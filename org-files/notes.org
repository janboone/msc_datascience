#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/stylesheet.css" />
#+Title: Notes Data Science for Economics
#+OPTIONS: toc:2 timestamp:nil toc:nil
#+STARTUP: inlineimages

* How are the weights in a neural network updated?

Consider the following simple classification problem. We have three points (0,1),(1,0),(1,1) with label/target equal to 1 and one point (0,0) with label equal to 0. We are looking for a line which separates these points.

That is, we want a line such that all points above the line have label 1 and the points below it have label 0. The line that is drawn in the figure is actually not right, so we need to update the "weights" of the line.

#+BEGIN_SRC jupyter-python :session py :kernel python3
import matplotlib.pyplot as plt
import numpy as np
inputs = np.array([[0,0],[0,1],[1,0],[1,1]])
targets = np.array([[0],[1],[1],[1]])
colors = []
for i in range(len(targets)):
    colors.append(['red','blue'][targets[i][0]])
plt.scatter(inputs[:,0],inputs[:,1],color=colors)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.plot([-0.03,1.03],[0.8,0.2])
plt.savefig('classification.png')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/f3a46b2368db195c30a6a0804114cd28921864b4.png]]

We write the line as $w_0 x + w_1 y = w_2$. Note that the line is not uniquely defined: any multiple of the coefficients gives us the same line. Hence, some normalization could be applied, but we ignore this for now.

Assume that $w_1 > 0$, then points above (below) the line feature $w_0 x + w_1 y > (<) w_2$ and we label these points 1 (0).

To use our geometric intuition, we write the equation for the line as:
\begin{equation}
\label{eq:1}
y = \frac{w_2 - w_0 x}{w_1}
\end{equation}
With this representation, we see that the incorrect classification of point (1,0) with label 0 is caused by the fact that the line is not steep enough. That is --assuming $w_1>0$-- we need to increase $w_0$.

Let $\hat t$ denote the activation or predicted label for an observation and $t$ the true label or target. Then the update rule can be written as
\begin{equation}
\label{eq:2}
w_0 = w_0 - \eta (\hat t - t)
\end{equation}
where $\eta > 0$ denotes the learning rate. For the point (1,0) this becomes
\begin{equation}
\label{eq:3}
w_0 = w_0 - \eta (0 - 1)
\end{equation}
Hence, the algorithm indeed raises $w_0$ in response to the incorrect classification of (1,0).

* Deep neural network

The network above had only 1 layer. We had inputs x, y and $-1$, where the latter is called "bias" in a neural network; in our figure above it is the constant in a line like $y = ax+b$.

Now let's introduce a second layer. This is usually called a hidden layer as it does not directly feature the inputs of the system, nor the outputs.

Assume that this second layer also has 3 nodes. We denote the weights from the input layer to the first hidden note by $(u_0,u_1,u_2)$. For the second and third nodes in the hidden layer we use weights resp. $(v_0,v_1,v_2),(w_0,w_1,w_2)$.

Hence, the first hidden node has value $u_0 x + u_1 y - u_2$ etc.

The third layer is the output layer which has only one node. If the value in this node is negative, we label the observation 0 and otherwise we label it 1.

The weights from the three hidden nodes to the final node are given by $(z_0,z_1,z_2)$. Hence, we can write the input of the final node as
\begin{equation}
\label{eq:4}
z_0(u_0 x + u_1 y - u_2) + z_1 (v_0 x + v_1 y - v_2) + z_2 (w_0 x + w_1 y - w_2)
\end{equation}
But this looks like a linear combination of $x,y,-1$. What is the use of the hidden layer if we end up with a linear combination of $x,y,-1$ anyway? Did we overlook something?
