{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/janboone/datascience_course/blob/master/Statistical_Hacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VDI4r3BYfGH"
   },
   "source": [
    "# Why statistical hacking?\n",
    "\n",
    "In order to get started on the \"big data\" part of this course, it is important that you have a good intuition about a number of statistical results that you have seen before in your statistics and econometrics courses. We go over these concepts by simulating our own data. This serves two goals:\n",
    "\n",
    "* you fully understand where the data comes from (you generated it yourself)\n",
    "* we train your programming skills in generating the data.\n",
    "\n",
    "Being able to generate your data using python simulations is a first step in understanding how to deal with \"real\" data. \n",
    "\n",
    "Based on [this lecture](https://www.youtube.com/watch?v=Iq9DzN6mvYA) and [this one.](https://www.youtube.com/watch?v=VR52vSbHBAk)\n",
    "\n",
    "If you like, there is also [free book](https://greenteapress.com/wp/think-stats-2e/) on this topic using python.\n",
    "\n",
    "We are going to discuss the following topics. First, as you probably know, estimated parameters have a distribution. To illustrate, you may recall that some statistics have a t-distribution. When you move into machine learning, the models get so complicated that there are no analytical results on the distributions of estimated coefficients. So how do you get a sense of uncertainty in that case? You simulate the distribution. Below we show this with examples where you actually know (or should know) what the relevant distributions are.\n",
    "\n",
    "Sometimes you only have a sample and no knowledge about the underlying model. Then you cannot simulate the distribution. But you can use the sample to learn something about the uncertainty underlying your parameters. This is called bootstrapping.\n",
    "\n",
    "In Economics we are not only interested in predicting variables (like economic growth or stock prices) but we also want to influence them through policy interventions. For this we need to know causal links between variables; not just correlations. Although many people have the intuition that it is better to control for more variables in  a regression, this intuition is actually not correct. By controlling for some variables, you actually mess up your interpretation of an effect in a regression.\n",
    "\n",
    "Finally, with \"big data\" it is very tempting to use a lot of variables in your models. You have got all these variables in your \"big data set\" so why not use them? This brings us to the concepts of over- and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacamp courses:\n",
    "\n",
    "https://www.datacamp.com/courses/statistical-simulation-in-python\n",
    "\n",
    "(https://www.datacamp.com/courses/deep-learning-with-keras-in-python)\n",
    "\n",
    "https://www.datacamp.com/courses/introduction-to-tensorflow-in-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VDI4r3BYfGH"
   },
   "source": [
    "# Loading packages\n",
    "\n",
    "To generate the data, we are going to use [tensorflow](https://www.tensorflow.org/). If you are running this in colab, install `tensorflow` and `linearmodels` by un-commenting the next cell.\n",
    "\n",
    "Run the other cell to import all the packages that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44376,
     "status": "ok",
     "timestamp": 1558779208617,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "mx-GwyO5avzp",
    "outputId": "2291d5d8-d323-41b1-926b-132c8c511cec"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.0.0-alpha0\n",
    "#!pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEGgjmMeX6U3"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import statsmodels.api as sm # check the error that cannot import name 'factorial' in from scipy.misc import factorial\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import altair as alt\n",
    "from linearmodels.iv import IV2SLS\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow import keras\n",
    "\n",
    "#alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQO97pjfeQOK"
   },
   "source": [
    "# Distributions of an estimator\n",
    "\n",
    "Let's start simple and consider a uniform distribution on $[0,1]$. We take a sample of size `sample_size` and calculate the mean $m$ of this sample. This variable $m$ is a sample statistic. We want to understand the properties of this sample statistic.\n",
    "\n",
    "In most courses that you have done, you will have used statistical theory here. E.g. what is the distribution of $m$?\n",
    "\n",
    "For simple models, like calculating the mean of a sample or doing an OLS regression, there is theory that describes what the distributions are of the mean or an estimated slope parameter. However, with big data techniques, you are going to use models where such theoretical results do not exist. So how do you figure out then what is happening?\n",
    "\n",
    "This is where the hacking comes in. The simple idea is: we program the statistical problem and then run it \"lots of times\", say 10,000 times. This gives us a distribution.\n",
    "\n",
    "## distribution of a sample average and sample standard deviation\n",
    "\n",
    "For the example of $m$, we do this in the next code cell.\n",
    "\n",
    "We draw `N_simulations` times a sample of `sample_size` and put this in a matrix (tensor) of size `N_simulations` by `sample_size`. Then we take the mean within a sample. Put differently, we take the mean over the second dimension (\"columns\") of the matrix. As python starts counting from 0, this means we take the mean over `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6palYMJded3Z"
   },
   "outputs": [],
   "source": [
    "N_simulations = 10000\n",
    "sample_size = 10\n",
    "simulated_data = np.mean(tf.random.uniform([N_simulations,sample_size],0,1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHe1np30bARr"
   },
   "source": [
    "The code above calculated `N_simulations` averages (of samples of size `sample_size`). \n",
    "\n",
    "**Question** Plot a histogram of `simulated_data` using `matplotlib`.\n",
    "\n",
    "**Question** When considering this distribution, which \"theorem\" is at work here?\n",
    "\n",
    "**Exercise** Play around with `sample_size` to see what the effect is of different values for this variable.\n",
    "\n",
    "**Exercise** Calculate the standard deviation of $m$; how does this depend on `sample_size`? Can you plot the standard deviation of $m$ against sample size? Which functional form is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1556181123319,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "oFLg0pyRf-rt",
    "nbgrader": {
     "checksum": "31ce2ccd05be64e434d19a4101d08333",
     "grade": true,
     "grade_id": "cell-c35a8216d1291f17",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "7173ccdc-8f1c-415b-b61d-b0bbe1d68dea"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "32d4ab969be5de1e8c66f6d3bd176cd7",
     "grade": true,
     "grade_id": "cell-2288157c46091176",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a90c39d4da02cc846bebf6e3baf809a4",
     "grade": true,
     "grade_id": "cell-dcce96dcaeb0093f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we considered the distribution of the sample mean $m$. But any parameter has a distribution; e.g. also the standard deviation of the sample $s$ has a distribution. Note that the standard deviation of the sample $s$ is **not** the same as the standard error $\\sigma/\\sqrt{n}$ which is the standard deviation of $m$'s distribution. Do not worry if you find the last sentence confusing in the beginning...\n",
    "\n",
    "**Question** Plot the distribution of $s$. What is the probability that $s<0.28$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b77bfcc06011426c3aa00a57b84bed6c",
     "grade": true,
     "grade_id": "cell-bbd11ce0c350942c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B1yOkLQ-ojtC"
   },
   "source": [
    "## distribution of a slope\n",
    "\n",
    "If we run a simple OLS (ordinary least squares) linear regression, we find the constant and the slope of this line. These parameters have also distributions! We can plot these distributions as well.\n",
    "\n",
    "We first generate the data with which we want to estimate a regression line.\n",
    "\n",
    "**Question** Explain the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dbanu7HXcD9q"
   },
   "outputs": [],
   "source": [
    "N_simulations = 10000\n",
    "sample_size = 20\n",
    "slope = 0.5\n",
    "constant = 1.0\n",
    "noise = 0.1\n",
    "simulated_x = tf.random.normal([N_simulations,sample_size])\n",
    "simulated_y = constant + slope * simulated_x + noise*tf.random.normal([N_simulations,sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vector `constants` and a vector `slopes`. For each of our simulations we add the resulting constant and slope to their resp. vectors.\n",
    "\n",
    "Note that the method `.numpy()` turns the tensorflow tensors into numpy arrays which are easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmNG9Mw8fwWT"
   },
   "outputs": [],
   "source": [
    "constants = []\n",
    "slopes = []\n",
    "\n",
    "for i in range(N_simulations):\n",
    "    model = sm.OLS(simulated_y[i,:].numpy(), sm.add_constant(simulated_x[i,:].numpy()))\n",
    "    results = model.fit().params\n",
    "    constants.append(results[0])\n",
    "    slopes.append(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbuZpg9Z1MMp"
   },
   "source": [
    "**Question** Plot the distribution of slopes and the distribution of constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 6835,
     "status": "ok",
     "timestamp": 1556181140121,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "GYgc5gxZgAmx",
    "nbgrader": {
     "checksum": "e746ee02651b0ee7e66147ecf45ad11e",
     "grade": true,
     "grade_id": "cell-32124b40f04fcbbb",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "9f74c1ad-2b14-473e-b842-d434c69aae57"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1556181152549,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "vrCBf-h5hX9D",
    "nbgrader": {
     "checksum": "1180cf32e8d2d801150e29c97228bf36",
     "grade": true,
     "grade_id": "cell-a15ff0e5de2ccdec",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "fe6ec456-0d47-40e7-d5c1-6e096afdc288"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXZHvqN71Y1k"
   },
   "source": [
    "Instead of plotting the slopes and constants separately, we can plot the lines that are induced by these distributions of the constant and the slope.\n",
    "\n",
    "**Quenstion** Plot in $(x,y)$ space, the distribution of lines that follow from these distributions of slopes and constants.\n",
    "\n",
    "**Question** For which values of $x$ is the uncertainty about $y$ the biggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 4074,
     "status": "ok",
     "timestamp": 1556181164969,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "roQh2rG0ihye",
    "nbgrader": {
     "checksum": "1139bf7567a43e4c50295dde4873be76",
     "grade": true,
     "grade_id": "cell-b9682c658983df70",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "6731be15-f494-4423-f7b6-8781c530a827"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWAjo1Clo1Hi"
   },
   "source": [
    "# Bootstrapping\n",
    "\n",
    "Sometimes we do not know what the underlying model is that generated the data. We only have the sample that we observed.  How do we proceed in this case if we want to test properties of the data? Here we can use bootstrapping.\n",
    "\n",
    "\n",
    "\n",
    "Suppose that we have two populations (e.g. men vs women; or in the test of a new drug: patients that got the treatment and patients that received the placebo). Denote these two samples by $A$ and $B$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1556181181292,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "L5dfK9IXsRb7",
    "outputId": "c91dba2c-ab20-4b93-e9c2-b026c009f2eb"
   },
   "outputs": [],
   "source": [
    "sample_size = 50\n",
    "delta = 0.95\n",
    "A = 10+tf.random.normal([sample_size])\n",
    "B = A*delta\n",
    "plt.hist(A,label='sample A')\n",
    "plt.hist(B,alpha=0.3,label='sample B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrA6njSSteju"
   },
   "source": [
    "**Question** Show that the average of $A$ is higher than the average of $B$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1556181184976,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "K9B8sNaltB1D",
    "nbgrader": {
     "checksum": "ead02dd9a6bf232d08b8482227a7c4c2",
     "grade": true,
     "grade_id": "cell-d72270b078e90ff6",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "cac9ed0b-b430-4b9e-f16c-d0945a137086"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJKjGsGit0x_"
   },
   "source": [
    "But is this difference in means a significant effect (forget for a second that you saw the code generating the data)?\n",
    "\n",
    "In your statistics class you have seen a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) to figure out whether this difference is significant or not. But we are going to use simulations to establish this.\n",
    "\n",
    "The null hypothesis that we want to test is: samples $A$ and $B$ are drawn from the same distribution.\n",
    "\n",
    "If this is true, than we can simply combine the $A$ and $B$ observations, reshuffle them and calculate the difference. We do this 10,000 times and get a distribution of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h27aRxCOwPYu"
   },
   "outputs": [],
   "source": [
    "AB = tf.concat([A,B],axis=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Draw samples of 50 $A$ and 50 $B$ observations out of $AB$. Calculate the difference of the means for these two samples. Then repeat this 10000 times.\n",
    "\n",
    "[hint: start by creating an empty list `differences`, this list will be filled with 10000 elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "4MBw5Li8w13q",
    "nbgrader": {
     "checksum": "c85eeaeafeb7d024c3ffaae97ffd36f9",
     "grade": true,
     "grade_id": "cell-59e539b1b17574d3",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the histogram of the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1556181222069,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "dIBLGlk_yeJ0",
    "outputId": "bfabd576-107c-48c3-a931-eae934149850"
   },
   "outputs": [],
   "source": [
    "plt.hist(differences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculate the fraction of the 10000 differences where this difference exceeds `observed_difference`. Is the observed difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1556181232398,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "NqBohKU1ygyI",
    "nbgrader": {
     "checksum": "de6d835b16208c4f8b09b5735658cdad",
     "grade": true,
     "grade_id": "cell-3e37bbf784ead353",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "d122f36a-320c-473f-f9f1-7aac4d58941a"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** In terms of the steps that you just programmed, formulate what a significant difference means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ede77946bb106177cbde29ce7f6c6831",
     "grade": true,
     "grade_id": "cell-b7d9daee78d52788",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWPW6HI2HcVm"
   },
   "source": [
    "# Doing your own OLS\n",
    "\n",
    "There exist many packages that can do OLS for you. But it is illustrative, to program your own OLS estimator, because then you can see what it does.\n",
    "\n",
    "We start really simple and then add some things to the estimator. This method of programming our own estimator we will use below again for estimators that you may not know yet.\n",
    "\n",
    "For OLS, we could just substitute the solution using matrix algebra: $\\beta = (x^Tx)^{-1}x^Ty$. But we want to use the optimization problem itself so that we can add things to it later on.\n",
    "\n",
    "First, we are going to generate our own data.\n",
    "\n",
    "Then we define a loss function (mean squared error) and minimize the loss by choosing a constant (`w[0]`) and slopes (`w[1],w[2]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "slope1 = -3.0\n",
    "constant = 1.0\n",
    "x1 = tf.random.normal([sample_size])\n",
    "x2 = tf.random.normal([sample_size])\n",
    "y = slope1*x1+constant+3*tf.random.normal([sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Define a function `loss` which takes as its argument the vector $w$. The first element of $w$ is the constant, the second element is the slope of our regression w.r.t. `x1`, the third element the slope w.r.t. `x2`. \n",
    "\n",
    "The function `loss` returns \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n (w_0+w_1 x_{1i} + w_2 x_{2i} - y_i)^2\n",
    "$$\n",
    "\n",
    "where $n$ denotes the sample size.\n",
    "\n",
    "**Question** Minimize the loss function. What values do you find for $w_0,w_1,w_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1556358574664,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "BWhR_jcFhmbe",
    "nbgrader": {
     "checksum": "4d4d99ed2184bc3a428836cd2b2206e7",
     "grade": true,
     "grade_id": "cell-e1603011322ed7b4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "b55ec367-f27b-4d0c-e5b4-f7f8fa83f502"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are ridge and lasso regressions?\n",
    "\n",
    "In machine learning, the focus is not so much on significance of estimated parameters (which we usually worry about) but on \"overfitting\". Overfitting is the problem where you seem to find a pattern in the data that is actually not there. It is related to the 5% significance that we worry about.\n",
    "\n",
    "![significant](https://imgs.xkcd.com/comics/significant.png)\n",
    "\n",
    "In case you need [an explanation](https://www.explainxkcd.com/wiki/index.php/882:_Significant).\n",
    "\n",
    "We will look more formally at overfitting below. The simple idea of ridge and lasso regressions is the following. We do not want the coefficients to be different from zero when, in fact, there is no relationship between the $x$ variable and $y$. This is sometimes referred to as \"we do not want the model to become too excited\".\n",
    "\n",
    "To avoid the model becoming too excited, we introduce a \"penalty\" on coefficients that are different from zero. The difference between ridge and lasso is the penalty function that is used.\n",
    "\n",
    "With ridge the penalty function is $\\sum_{i=1}^p w_i^2$ and with lasso it is $\\sum_{i=1}^p |w_i|$. The penalty function is multiplied by \"hyperparameter\" $\\lambda>0$ and added to the loss function that we used below.\n",
    "\n",
    "Note that this procedure only makes sense if the variables are normalized. The $x$ variables are generally standardized (mean 0, standard deviation equal to 1; as they are in our case). The $y$ variable is centered: mean 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Create a variable `y_centered` which has mean 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "be0a50dd0668d76c7ab93a6ffb159b9f",
     "grade": true,
     "grade_id": "cell-7e32ae5b497a36e4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Define a function `loss_ridge` with parameters `w` (vector of coefficients) and $\\lambda$. \n",
    "\n",
    "What coefficiens do you find with $\\lambda =1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0aeec9db071be8cf1860bf3b19f10f87",
     "grade": true,
     "grade_id": "cell-b76a71a2f36fe826",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Define a function `loss_lasso` with parameters `w` (vector of coefficients) and $\\lambda$. \n",
    "\n",
    "What coefficiens do you find with $\\lambda =1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "210bc07739f2c63bf62ebf8c69ac73b3",
     "grade": true,
     "grade_id": "cell-a275dbe1060c40bd",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Psozyj3XcdA8"
   },
   "source": [
    "# Causality\n",
    "\n",
    "As you know, multiple regression is about correlations, not about causality. However, in economics we do want to know causal effects as well, e.g. when we do policy analysis. If you want to advice the government to do policy $X$ because it will increase $Y$, you need to be sure that there is a causal effect between $X$ and $Y$.\n",
    "\n",
    "Hence, often people try to interpret the outcomes of a regression as causal relationships. When doing this, there are a number of mistakes that you can make. Here we are going to look at three of these mistakes:\n",
    "\n",
    "* the fork\n",
    "* the pipe\n",
    "* the collider\n",
    "\n",
    "Each of these three mistakes will be illustrated by a DAG (\"a figure with variable names and arrows between them\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTxJRtkedmgi"
   },
   "source": [
    "## The fork\n",
    "\n",
    "This is the situation where there is no relation between the variables $X$ and $Y$. However, both are influenced by a third variable $Z$. \n",
    "\n",
    "\n",
    "\n",
    "![fork](./Fork.png)\n",
    "\n",
    "\n",
    "This is the situation you probably know best. There is a missing variable that is important in the relation between $X$ and $Y$. Running a regression with only $Y$ and $X$ leads to a bias in the coefficient of $X$. In the first case we consider this is extreme in the sense that the effect of $X$ on $Y$ is zero (there is no such effect) and yet you find a significant coefficient.\n",
    "\n",
    "Doing this regression from $X$ on $Y$ seems to suggest a causal effect of $X$ on $Y$. This could misleadingly convince people that a policy that affects $X$ directly will also affect $Y$.\n",
    "\n",
    "However, using both $X$ and $Z$ as explanatory variables in the regression will show that there is no significant effect from $X$ on $Y$ *controlling for* $Z$.\n",
    "\n",
    "Hence, with a fork it is important to think about possible variables $Z$ and include them in your regression.\n",
    "\n",
    "**Question** Generate data for the variables $X,Y,Z$ such that there is a fork\n",
    "\n",
    "**Question** Put this data in a pandas dataframe.\n",
    "\n",
    "With the dataframe, it is easy to run the regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "abe2425787542903a809011aa293849a",
     "grade": true,
     "grade_id": "cell-29d4b42333f2e8d8",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use [statsmodels](https://www.statsmodels.org/stable/index.html) to run the regression from `x` on `y`.\n",
    "\n",
    "Note that we just use `statsmodels` here without giving you a full explanation of its syntax. Although the commands are intuitive enough, you may want to check the [statsmodels website](https://www.statsmodels.org/stable/index.html) to understand things. Do not read everything on this website (that would be too time consuming), just browse through it and find the things that you need. \n",
    "\n",
    "This is good practice for later when new packages are developed and you want to quickly start using a new package. A combination of browsing through the documentation and using some \"trial and error\" allows you to quickly learn a package and see whether it is useful for you.\n",
    "\n",
    "As you can see there is a negative correlation between `x` and `y` and the coefficient is \"highly\" significant. However, we generated the data ourselves (above) and we know that there is no direct effect from $x$ on $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fork_incorrect = smf.ols('y ~ x', data=df_fork).fit()\n",
    "results_fork_incorrect.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Run the same regression but now with both `x` and `z` as explanatory variables. What happens to the coefficient of `x`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "837b30d433e352e07a77de4a5a22bdf5",
     "grade": true,
     "grade_id": "cell-0b93b5501654593e",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we assumed that the effect of `x` on `y` is zero. Now we consider a generalized version.\n",
    "\n",
    "**Exercise** Assume that the direct effect of `x` on `y` is linear with slope equal to 0.5 (not 0.0 as above). The other effects are the same as above. Generate the data and create a dataframe `df_fork_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d5285006a91c85e3082e1e68625b04e9",
     "grade": true,
     "grade_id": "cell-65349680f13e58b1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Run the same regressions that we did above using `statsmodels`. Discuss the coefficients that you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "30b1d1e4f8febaef94f0d5383740494e",
     "grade": true,
     "grade_id": "cell-6b9a665e4229ad06",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9e8ec06fa0fcade414dd83c162b7f91",
     "grade": true,
     "grade_id": "cell-8c771dc780706bfe",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e14f37b2a948caa33f423b5ec2149c31",
     "grade": true,
     "grade_id": "cell-e18f24cbf2d8775c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qs3nhFBHdnKF"
   },
   "source": [
    "## The pipe\n",
    "\n",
    "We are interested in the causal effect of $X$ on $Y$. In reality this effect is mediated by $Z$.\n",
    "\n",
    "If we condition on $Z$, the regression suggests that there is no causal impact of $X$ on $Y$. It looks as if we just \"solved a fork\". From the data alone we do not know whether there is a fork or pipe. We need a model to separate the two situations.\n",
    "\n",
    "![pipe](./Pipe.png)\n",
    "\n",
    "Let us first look at a data generating process that leads to a pipe. Then we give an economic story of how a pipe can arise.\n",
    "\n",
    "**Question** Copy/paste the code from the fork example above. Adjust the equations for `x,y,z` such that we have a pipe and not a fork. That is, make sure that `x` affects `z` which then affects `y`. Then create a dataframe `df_pipe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4aca8a6f5719235bed1a62f783e25786",
     "grade": true,
     "grade_id": "cell-b4b8112c50bf1b99",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Run the regression from `x` on `y` directly and the regression where, in addition to `x`, we also control from `z`. Note that the `incorrect` regression has both `x` and `z` this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cf33f696fb5550a0db6a7f70f95a4812",
     "grade": true,
     "grade_id": "cell-f1158e36fbed0608",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba2dde7696a9a3fe64910cc46ccec06f",
     "grade": true,
     "grade_id": "cell-fd087e63d636fd43",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Discuss the relevant coefficients that you find above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4568d6e4c2ec586f84b4205f9047877f",
     "grade": true,
     "grade_id": "cell-8b8ecae7063a1ac3",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In economics an example could be the effect of schooling on poverty. Poverty can be reduced by increasing income but also by avoiding mistakes in expenditures (e.g. not taking out loans with absurdly high interest rates). Suppose the former mechanism is the main one. Then looking at the effect of schooling on poverty *controlling* for income would suggest that schooling has hardly an effect on poverty. However, there is a strong causal effect from schooling on poverty and it happens through income.\n",
    "\n",
    "By controlling for income, you \"control away\" the real causal variable which is schooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42ZPCNqMdm2l"
   },
   "source": [
    "## The collider\n",
    "\n",
    "Many people have the intuition that adding a control variable to a regression (especially when it turns out to be signficant) is a good idea. However, this intuition is wrong if it is applied without thinking. The pipe above is a first example where this reasoning is incorrect. The collider is a second case.\n",
    "\n",
    "We use here the education example by Richard McElreath to illustrate this point (see [Richard McElreath\n",
    "'s lecture 6](https://www.youtube.com/watch?v=l_7yIUqWBmE)). Consider the educational achievements of three generations in one family: the grandparent, the parent and the (grand)child. We are interested in the effect of the grandparent's education on the educational achievement of the grandchild. \n",
    "\n",
    "Let's denote the child's educational achievement by $Y$ (the variable we are interested in) and the grandparent's education $X$. We want to understand the effect of $X$ on $Y$. Clearly there is the following path: more educated grandparent leads to more educated parent ($Z$) which, in turn, leads to more educated grandchild. The thing we are interested in is whether there is a direct effect from the grandparent on the child, *controlling* for the parent's education.\n",
    "\n",
    "![collider](./Collider.png)\n",
    "\n",
    "We know that the parent's education has a positive effect on the child's achievement. A more educated parent will tend to stimulate their children more, can help with homework etc. In the data that we generate below, we assume that $Z = X + ...$ and $Y = Z + ...$. That is, we assume that the parent's education feeds one-to-one into the child's education. \n",
    "\n",
    "Is it the case that on top of this effect there is a separate grandparent effect? E.g. because the grandparent is babysitting and a more educated grandparent reads Hamlet with the 5 year old grandchild. That is fun and can boost the child's educational achievement.\n",
    "\n",
    "Another important component of education is the neighborhood where the child grows up. We denote this variable $U$. In the code we assume that the parent and child grow up in the same neighborhood (have the same $U$ effect).  $U$ is drawn from a standard normal distribution and the effect on education is given through a multiplication by the factor `alpha`.\n",
    "\n",
    "In addition to this, we assume that there is a random component as well which differs between parent and child.\n",
    "\n",
    "![collider](./Collider2.png)\n",
    "\n",
    "The code below, generates this data and a dataframe `df_collider` that stores this data.\n",
    "\n",
    "Look carefully at the code: is there a direct effect from the grandparent to the grandchild's educational achievement?\n",
    "\n",
    "We generate two dataframes: `df_collider` is the one that is observed and used in the analysis by the researcher. \n",
    "\n",
    "We will use `df_collider_2` to illustrate why one can get incorrect results from analyzing `df_collider`. The problem is that the researcher does not have access to `df_collider_2` but we can analyze `df_colldier_2` for \"educational purposes\" to illustrate why problems arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1556188665098,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "YbGfWVekX-8G",
    "outputId": "543372db-4639-456d-c1d1-d192de042d09"
   },
   "outputs": [],
   "source": [
    "N_observations = 5000\n",
    "alpha = 0.5\n",
    "X = tf.random.normal([N_observations])\n",
    "U = tf.random.normal([N_observations])\n",
    "Z = X + alpha*U+0.4*tf.random.normal([N_observations])\n",
    "Y = Z + alpha*U+0.4*tf.random.normal([N_observations])\n",
    "\n",
    "df_collider = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z})\n",
    "df_collider_2 = pd.DataFrame({'X':X, 'Y':Y, 'Z':Z, 'U':U})\n",
    "print(df_collider.head())\n",
    "print(df_collider_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2ED-R_JNHfD"
   },
   "source": [
    "**Question** Find the (direct) effect of the grandparent ($X$) on the grandchild's educational achievement ($Y$) conditional on the parent's education ($Z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1556188667363,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "st0jhpiEYD3z",
    "nbgrader": {
     "checksum": "c8a863d63d070edca9608882e70e5569",
     "grade": true,
     "grade_id": "cell-c45ac0c65de5b1c1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "c9ffbeb6-4dbd-46a7-bbbe-735b9f1977ed"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlVQK6LCNroW"
   },
   "source": [
    "The effect of the grandparent's educational achievement ($X$) on the child's achievement is negative and significant. \n",
    "\n",
    "**Question** Where does this negative efffect come from? How is this even possible?\n",
    "\n",
    "**Question** Find the direct effect of $X$ on $Y$. Which sign does this effect have? What is the interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1556188668741,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "7WNHOCh1YGWt",
    "nbgrader": {
     "checksum": "d61c8984f5b0d66571e025d2cb91eee4",
     "grade": true,
     "grade_id": "cell-6ec3c72bc5162325",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "8804e459-cd69-4c38-fdd6-32a972e898ad"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2f707d06b21d14ad331d937a3418b6cd",
     "grade": true,
     "grade_id": "cell-29865003fc27a16a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILo37WUppWMw"
   },
   "source": [
    "To understand the negative effect of $X$ on $Y$, let's think about what it means to \"control for parental education $Z$\". \n",
    "\n",
    "To visualize what happens, we condition on $Z$ by focusing on a narrow band of $Z$ observations. In other words, we condition on $Z$ by focusing on a subsample where the $Z$ values are (almost) the same. We do this by creating a column `selected` in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zzc8Qj5djoaI"
   },
   "outputs": [],
   "source": [
    "df_collider['selected'] = np.abs(df_collider.Z)<0.05\n",
    "df_collider.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1556188670972,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ythl_bmoiTVc",
    "outputId": "4e49ce97-594b-4cdc-85ac-9059901ff3d3"
   },
   "source": [
    "**Question** Plot $X$ against $Y$ for all the data (in blue) and for the values of $Z$ where $|Z|<0.05$ (in orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9ee89d83ca4580b7a183688dfe7cd7f2",
     "grade": true,
     "grade_id": "cell-58a4e7a604ca6472",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMwDuCmh1LCt"
   },
   "source": [
    "**Question** Give the intuition why there is a negative correlation between $X$ and $Y$ with the orange dots. [hint: what do you know about orange dots with low $X$ and about orange dots with high $X$?]\n",
    "\n",
    "If you cannot answer this question yet, consider the following interactive graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1552ba051245b5340f54b1a1d084c043",
     "grade": true,
     "grade_id": "cell-a263f2f972584d1d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOp8m46p14Au"
   },
   "source": [
    "To better understand what is happening here, we are going to use `df_collider_2`. In particular, in the combined figure below, we again provide a scatter plot of `X` vs `Y`. Now the color indicates the level of `Z` associated with the observation; darker colors indicate higher values of `Z`. The size of the point (circle) indicates the level of `U` (that the researcher cannot see in `df_collider`).\n",
    "\n",
    "To better illustrate what is happening, we use an interactive plot made with [altair](https://altair-viz.github.io/).\n",
    "\n",
    "To condition on `Z` (as we do in a multiple regression), we can drag a rectangle in the bottom histogram. That is, drag the rectangle so that you \"capture\" a bar in the histogram. This selection of $Z$ turns blue and you can drag the selection up and down (conditioning on different values of $Z$).\n",
    "\n",
    "Move this selection up and down and see what happens. In particular:\n",
    "* how does `Z` vary in the figure? E.g. where are the high values of `Z`? What is the interpretation of this?\n",
    "* for a selection of `Z` in the bottom histogram, what is the relation between `X` and `Y`?\n",
    "* for this selection, how does `U` vary in the figure? That is, for a given selection of $Z$ what is the size of the circles? What is the interpretation?\n",
    "\n",
    "\n",
    "**Question** Give the intuition why there is a negative correlation between `X` and `Y` for a selection of `Z`; that is, conditional on `Z`.\n",
    "\n",
    "**Exercise** What is the risk of saying: I have run a regression and controlled for all effects that I had variables for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1556195913155,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "VC8-77Rf12u5",
    "outputId": "96cd0c7c-22ba-4fc4-f98c-513d4b8e2f23"
   },
   "outputs": [],
   "source": [
    "interval = alt.selection_interval(encodings=['y'])\n",
    "\n",
    "fig = alt.Chart(df_collider_2).mark_point().encode(\n",
    "  x='X',\n",
    "  y='Y',\n",
    "  color=alt.condition(interval, 'Z', alt.value('lightgreen')),\n",
    "  size = 'U'\n",
    ").properties(\n",
    "    selection=interval\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hist = alt.Chart(df_collider_2).mark_bar().encode(\n",
    "    x='count()',\n",
    "    y=alt.Y('Z',bin=True),\n",
    "    color=alt.condition(interval, 'Z', alt.value('lightgrey'))\n",
    ").properties(\n",
    "    selection=interval\n",
    "\n",
    ")\n",
    "\n",
    "chart_collider = fig & hist\n",
    "\n",
    "chart_collider.save('ChartCollider.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"840\" height=\"800\" src=\"./ChartCollider.html\" frameborder=\"0\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe width=\"840\" height=\"800\" src=\"./ChartCollider.html\" frameborder=\"0\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "426c9afbd4d00cc6a4243bef684e3ae2",
     "grade": true,
     "grade_id": "cell-250fcf8087509d00",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b12b167e2a8a2b42c06e4702da54851c",
     "grade": true,
     "grade_id": "cell-a11ada232f82a470",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a7c6e427a30aa781e843d6b841d94662",
     "grade": true,
     "grade_id": "cell-6e88fa733f26814a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jyve80prPAB2"
   },
   "source": [
    "# Some background on tensors\n",
    "\n",
    "In the past, when you worked with data, you would usually have 2-dimensional data: columns for the different variables and rows for the different observations. That is, the data is represented as a matrix.\n",
    "\n",
    "However, \"big data\" is not necessarily structured in such a neat way. Data can have higher dimensions. We will start with data on images. An image has two dimensions of itself. Hence a dataset with images has 3 dimensions: one dimension for the identifier of the image and then x-y coordinates for the image itself.\n",
    "\n",
    "If the dataset consists of movie-clips, we have an additional dimension: time.\n",
    "\n",
    "In economics, when we look at GDP, our observation would be GDP of the UK in 2010, GDP of the UK in 2011, GDP of Belgium in 2010 etc. This can be represented as a matrix.\n",
    "\n",
    "But now suppose we believe the development of GDP is important. Our observation would be GDP of the UK in 2010-2018. This gives 3 dimensional data. The observation is GDP in the period. \n",
    "\n",
    "To deal with higher dimensional data, we need \"tensors\". A tensor generalizes a matrix to higher dimensions.\n",
    "\n",
    "We will now have a technical interlude to explain what tensors are and what you can do with them. We describe some functions that can be applied to tensors. To motivate these functions, we will build our first neural network. The point here is not so much to explain how neural networks work but more to show you that the functions we consider are going to be useful later on when we will try to understand neural networks more deeply.\n",
    "\n",
    "Machine learning packages tend to have a \"backbone\" that implements tensors. E.g. Keras allows you to work with either [Theano](http://www.deeplearning.net/software/theano/) or [tensorflow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOGCDKigifED"
   },
   "source": [
    "## Tensors\n",
    "\n",
    "As mentioned, in most of the empirical analyses that you have done, an observation is a one dimensional vector. E.g. you have data on inflation, unemployment, gdp growth for country-year combinations. Then for the UK in 2000, our data  consists of the one dimensional vector `[inflation, unemployment, gdp growth]`. If you have 100 observations like this, you can represent them in a matrix. Each row is an observation and the columns will be `[coutry, year, inflation, unemployment, gdp growth]`. Hence we have a two dimensional dataset with 100 rows and 5 columns.\n",
    "\n",
    "In big data, there are usually higher dimension observations. One of the \"classic\" datasets in machine learning is [the MNIST dataset.](https://en.wikipedia.org/wiki/MNIST_database) This dataset consists of handwritten numbers and their corresponding label (e.g. when the handwritten number is 5, the label for this image `5`). We will see an example shortly.\n",
    "\n",
    "The point is that a handwritten number is a two dimensional observation. For each x-y comination, we get the grey-scale of this point/pixel. To illustrate this, consider the 5th image from the training dataset (python starts numbering at 0); we plot the handwritten image (which is 2 dimensional) and print its label.\n",
    "\n",
    "This data is already split in a train and test set. Something we will elaborate on below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1556370513629,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "SmEybeeliYi8",
    "outputId": "813f060a-15c5-4264-88cd-0a38cffb5c7d"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1556370516249,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "8SSJCxWMkQe6",
    "outputId": "25b3b30f-7ad0-44bd-f7fe-e4c1e133af55"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_images[4],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(train_labels[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to \"see\" what the data look like, uncomment the next cell and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4L5zmRgqMJo"
   },
   "source": [
    "Indeed, the handwritten number is 9, which equals its label.\n",
    "\n",
    "What are the dimensions for this dataset. Let's use `shape` to find out. Friendly warning, when you go into machine learning, you will be using `shape` a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1556282456035,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "WOg2L1fAkhZv",
    "outputId": "1ad602f3-b610-452f-a895-a7d2195a8605"
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oj8-DKccqmca"
   },
   "source": [
    "That is, we have 60,000 images in the train-set and each image is of the form 28 by 28 (pixels).\n",
    "\n",
    "As an illustration, the dimensions of the first image in the dataset `train_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1556282548428,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "gXigpmrjqbW7",
    "outputId": "82312993-9953-49d6-c469-bd592ff2e944"
   },
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pq8zXvScr19o"
   },
   "source": [
    "## Tensors in numpy\n",
    "\n",
    "As mentioned, well known machine learning backends are theano and tensorflow. For reasons that we do not worry about here, these are not immediately straightforward to work with (but we have used some tensorflow already above). We can play around with dimensions in \"good old\" numpy as well. Properties that we can use in numpy, like broadcasting, can be used in theano and tensorflow as well.\n",
    "\n",
    "As numpy is \"more direct\" to work with, we will practice this with numpy.\n",
    "\n",
    "**Remark** Although you are always encouraged to play around with code to see what happens, this is especially the case with tensors. To familiarize yourself with tensors, change the code that we use below; e.g. create a 10 dimensional vector out of the data. You can check for yourself whether the code generates what you expect it to generate. If not, and you cannot figure out what happens, ask us!\n",
    "\n",
    "First, we create the vector `x` with 100 random numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HmqgKGEqx6c"
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0,1,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yd-M7PXttRW_"
   },
   "source": [
    "**Question** Use `shape` to find the shape of the vector `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1556283042608,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1tqdba21soaO",
    "nbgrader": {
     "checksum": "0ec3498c73e7c86bd1261a90eee68b98",
     "grade": true,
     "grade_id": "cell-64e1f3c7866d8945",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "5eec91a3-50ad-4ac4-b1e4-87acddd5ed58"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48L7R9QvtXaF"
   },
   "source": [
    "As you can see, `x` has 100 elements in one dimension and no other dimension. Put differently, `x` is one-dimensional as a tensor.\n",
    "\n",
    "This we can verify using `ndim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1556283118592,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ZHoMyyxhspAB",
    "outputId": "1998c7d9-72a8-4988-9d84-4ff7ea900936"
   },
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDNJKPz0thFs"
   },
   "source": [
    "**Do not panic:** The following sentence will be a bit confusing when you first read it, but you will get used to it later. If you would plot the vector `x` it would be a *vector* in a 100-dimensional space. Hence it is a 100 dimensional vector, but a 1 dimensional tensor.\n",
    "\n",
    "To understand this better, let's consider a two dimensional (2D) tensor. This is what we usually call a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wG27xM_s9GN"
   },
   "outputs": [],
   "source": [
    "x2 = x.reshape(25,4)\n",
    "print(x2.ndim)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By evaluating `x2` we see that this indeed \"looks like\" a matrix. \n",
    "\n",
    "In terms of python, we can see the two dimensions as follows:\n",
    "* each row has four elements (columns) separated by comma's (`,`) in between square brackets, like `[1,2,3,4]`; \n",
    "* then between square brackets, we have 25 of such rows; again rows separated by comma's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1556283467537,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "CkSfmwM_uNpB",
    "outputId": "6451e1d3-7504-45cf-c488-0fa2c262f5af"
   },
   "source": [
    "Note how the `reshape` method turns the 100 elements of the 1-dimensional vector `x`into a matrix with 25 \"rows\" and 4 \"columns\". This matrix is 2-dimensional.\n",
    "\n",
    "**Question** Create a 3-dimensional vector `x3` out of `x` with shape 4 by 5 by 5. Check the dimensions and shape of `x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "so_DNWlEuTow",
    "nbgrader": {
     "checksum": "00cdc0f16a3accfa4508f9355ad43af2",
     "grade": true,
     "grade_id": "cell-d8b0878a080c42f5",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1eB-x0Lu0R7"
   },
   "source": [
    "One way to think about `x3` is as four $5*5$ images. The numpy representation of this is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1388,
     "status": "ok",
     "timestamp": 1556283635679,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "e4dvF-EiuwKo",
    "outputId": "3f4e828f-e2d3-41c7-ee87-ca3fd53106ff"
   },
   "outputs": [],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6STQAVi9vhlt"
   },
   "source": [
    "With `np.newaxis` we can add a dimension to a vector without adding or re-arranging data. This works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhEGTMQ3u7JC"
   },
   "outputs": [],
   "source": [
    "x4 = x3[:,:,:,np.newaxis]\n",
    "print(x4.ndim)\n",
    "print(x4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUJF_kDIv6jg"
   },
   "source": [
    "Now if you evaluate `x4`, it looks a lot like `x3`. But if you look carefully there is an additional pair of square brackets `[]`. \n",
    "\n",
    "You may wonder why it is useful to add a `newaxis` to a tensor. Actually, one good reason for this is broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXi6_Jupv6rz"
   },
   "source": [
    "## Broadcasting\n",
    "\n",
    "Broadcasting is one of the things in python that are rather complicated to understand at first, but which is extremely useful and powerful once you \"get it\". Let's start with an example.\n",
    "\n",
    "Suppose you have estimated a fixed effect model. In your data there are i = 0,1,...,9 individuals and t = 0,1,2 periods. The vector `I` consists of 10 individual fixed effects and the vector `T` has 3 time fixed effects. Hence, our prediction for indiv. `i` in time period `t` (ignoring other explanatory variables) is $y_{it}=I_i+T_t$. \n",
    "\n",
    "How can we calculate the vector `y`? When you first think of this, you may have ideas for a \"double loop\": one loop over $i$ and one over $T$. In fact no loops are necessary at all, making the code \"very readable\".\n",
    "\n",
    "We will define the vectors $I$ and $T$ in such an (artificial) way that you can immediately verify whether we get the sum of the effects right.\n",
    "\n",
    "**Question** What do the vectors $I$ and $T$ look like? Can they be added in this way?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_z7TqTLvy7W"
   },
   "outputs": [],
   "source": [
    "I = np.arange(0,100,10)\n",
    "T = np.arange(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1556284277690,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "-B3wpCzkxXdV",
    "nbgrader": {
     "checksum": "0f0c03fe202678152393706d4ebdfdc6",
     "grade": true,
     "grade_id": "cell-b24ab095888e7da7",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "07a3d48d-a265-4b4f-94ab-39cc2e666412"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1556284280654,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Yo8-de_3xYGS",
    "nbgrader": {
     "checksum": "a652e9aab62168a1eab20c8a723be1c0",
     "grade": true,
     "grade_id": "cell-e90d145d3d54f5e4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "83819535-3f13-4a36-8a8f-39eac2c14b25"
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aebe3ad980f24d41d23b220938ba2a68",
     "grade": true,
     "grade_id": "cell-a193fdfaa970694c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAOnRrTJxbaM"
   },
   "source": [
    "Because of the simplistic numbers that we have chosen, it is easy to see that e.g. $y_{91} = 91$ with $i=9$ and $t=1$. \n",
    "\n",
    "We will use the \"broadcasting trick\" and check whether our trick works. Then we explain broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eua1v545xY1O"
   },
   "outputs": [],
   "source": [
    "y = I[:,np.newaxis]+T[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1556284519887,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "J_V75rdwyF-u",
    "outputId": "9d1c0348-702b-4107-f86c-639b7e053dbd"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have worked!\n",
    "\n",
    "To understand what happens here, evaluate `I[:,np.newaxis]` and `T[np.newaxis,:]` separately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1556284570652,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bPxpgAr_yGcR",
    "nbgrader": {
     "checksum": "5fad07418c1e0ea3400e856fe2b8c404",
     "grade": true,
     "grade_id": "cell-446e11a2971773b4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "88a9ef5c-984f-4da9-a12d-b8b39a622340"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1556284579224,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bhOoGx2wyfoW",
    "nbgrader": {
     "checksum": "a60ac7997aedb5a87c129774f43d5629",
     "grade": true,
     "grade_id": "cell-a69907c9b62016af",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "3f0b4c56-5baa-478b-d870-e0ca4e3a2efb"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw4T0tQWyi60"
   },
   "source": [
    "In words, `I` is a column vector\" only one column and 10 rows and `T` is a row vector: 3 columns and only 1 row.\n",
    "\n",
    "**Question** Check the shape of these vectors to see whether this description is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b9eeb75a3316cefd3ba6f9f39d61132c",
     "grade": true,
     "grade_id": "cell-d98555a7f48a8f55",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vw4T0tQWyi60"
   },
   "source": [
    "Adding these two vectors together using broadcasting leads to an element by element addition of the vectors. \n",
    "\n",
    "In fact, we have used a simple form of broadcasting before in our python classes. See, for instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGrVDEWryhtm"
   },
   "outputs": [],
   "source": [
    "T + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simple since 1 has no dimension.\n",
    "\n",
    "Read [this article on broadcasting](https://docs.scipy.org/doc/numpy/user/theory.broadcasting.html#array-broadcasting-in-numpy) and [this one](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) to learn more about it.\n",
    "\n",
    "To see whether two tensors can be added, we start looking at the end. In particular, as the latter article states, when \"operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n",
    "\n",
    "1. they are equal, or\n",
    "2. one of them is 1\"\n",
    "\n",
    "Let's consider a number of examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0,1,size=100)\n",
    "y = np.random.normal(0,1,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which of the following can be added? First, think about it. Then check by running the code in a cell.\n",
    "\n",
    "* `x+y`\n",
    "* `x.reshape(10,10)+y`\n",
    "* `x.reshape(10,10)+y.reshape(4,5,5)`\n",
    "* `x.reshape(10,10)+y.reshape(10,10,1)`\n",
    "* `x.reshape(10,10)+y.reshape(10,10,1)`\n",
    "* `x.reshape(10,10)+y.reshape(5,10,2)`\n",
    "* `x.reshape(10,10)+y.reshape(100,1,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99e8aa1e67c1f89cd51d45c301a1e60f",
     "grade": true,
     "grade_id": "cell-f52edff65348e988",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3lIIQBPSTWZ"
   },
   "source": [
    "## Slicing and (fancy) indexing\n",
    "\n",
    "Sometimes you want to have a subset from your data; e.g. to divide your data in a train and test set.\n",
    "\n",
    "In python, subsets are created using slicing.\n",
    "\n",
    "To see how slicing works, let's create an \"easy\" matrix to work with.\n",
    "\n",
    "**Question** create a matrix `x` of the form:\n",
    "\n",
    "```\n",
    "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10],\n",
    "       [ 11,  12,  13,  14,  15,  16,  17,  18,  19,  20],\n",
    "       [ 21,  22,  23,  24,  25,  26,  27,  28,  29,  30],\n",
    "       [ 31,  32,  33,  34,  35,  36,  37,  38,  39,  40],\n",
    "       [ 41,  42,  43,  44,  45,  46,  47,  48,  49,  50],\n",
    "       [ 51,  52,  53,  54,  55,  56,  57,  58,  59,  60],\n",
    "       [ 61,  62,  63,  64,  65,  66,  67,  68,  69,  70],\n",
    "       [ 71,  72,  73,  74,  75,  76,  77,  78,  79,  80],\n",
    "       [ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90],\n",
    "       [ 91,  92,  93,  94,  95,  96,  97,  98,  99, 100]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e254671f0b8e973d4a0bdbbcf03807c5",
     "grade": true,
     "grade_id": "cell-48d50fcfc8ef6c2c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with single elements, to make sure we understand indexing of an array.\n",
    "\n",
    "**Question** Use indexing on `x` to get:\n",
    "* `55`\n",
    "* `1`\n",
    "* `100` (and do this in two different ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b795b3235735db97adfaf52f2568e5a7",
     "grade": true,
     "grade_id": "cell-0853b1d327bac0b1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want a series of elements. For this we use slicing notation. To get the first row, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we take the first element in the first dimension (a row) and then all elements (`:`) in the other dimension (columns).\n",
    "\n",
    "**Question** Give the fifth column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d8a78a5c7c67ff98583a5773bf652fcf",
     "grade": true,
     "grade_id": "cell-880c9ec43735a8d9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To get the second half of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get every other element from this part of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,5::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the top left 2*2 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:2,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to get the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[range(10),range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What does the following slicing operation do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1::2,1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5fcc3e528db7c942efa11de6600492dc",
     "grade": true,
     "grade_id": "cell-9bf6497bae9d44f7",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Use slicing to get the following:\n",
    "* `[81, 82, 83, 84, 85, 86, 87, 88, 89, 90]`\n",
    "* `[ 3, 13, 23, 33, 43, 53, 63, 73, 83, 93]`\n",
    "*\n",
    "```\n",
    "[[56 57]\n",
    " [66 67]]\n",
    "```\n",
    "* \n",
    "```\n",
    "[[23 25 27]\n",
    " [43 45 47]\n",
    " [63 65 67]]\n",
    "```\n",
    "\n",
    "\n",
    "* the first row in reverse order: `[10  9  8  7  6  5  4  3  2  1]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[8,:])\n",
    "print(x[:,2])\n",
    "print(x[5:7,5:7])\n",
    "print(x[2:8:2,2:8:2])\n",
    "print(x[0,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a condition to select elements from a tensor. That is, we create a boolean mask (with `True` and `False`) and then only the `True` elements are selected. This is called fancy indexing.\n",
    "\n",
    "Let's start simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(5)\n",
    "mask = test > 2\n",
    "test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to define the `mask` separetely, the following works as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Select from `x` the values between 50 and 60.\n",
    "\n",
    "[hint: use `&` as `and`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[(x>50)&(x<60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two tensors $x,y$ have the same dimensions, we can select from $x$ based on a condition for $y$.\n",
    "\n",
    "To illustrate this, we create three vectors `x,y,z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=10)\n",
    "y = np.random.normal(size=10)\n",
    "z = np.random.normal(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot `x` against `y` and color points with $z>0.5$ orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = z > 0.5\n",
    "plt.scatter(x,y,label='$z \\leq 0.5$')\n",
    "plt.scatter(x[mask],y[mask],label='$z>0.5$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0rKwqdCShHm"
   },
   "source": [
    "## First neural network\n",
    "\n",
    "As an application of our tensors, let's consider a neural network. Let's not worry about the syntax of the model. Just note that as input we use the shape of a figure: 28 by 28 pixels. Hence, the input (\"x\" variable) is here a 2 dimensional tensor.\n",
    "\n",
    "Below we will come back to `relu` and `softmax`.\n",
    "\n",
    "More information about the optimizer `adam` can be found [here.](https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hZyoNwm6Je2"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate ('fit') the model on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23166,
     "status": "ok",
     "timestamp": 1556370965688,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "lRi2WhVB6J3i",
    "outputId": "94eeb1bc-c1ad-4a6f-d1de-29afca2d5d03"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we check the acciracy of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1556370974512,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "c-0AF9lZ6J_1",
    "outputId": "6f1e7863-2b9f-4d49-8d77-246e0ae5b1d9"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for the test set and consider explicitly the prediction for the first image (index 0). This prediction gives the probability for each possible label 0-9. The probability on label \"7\" equals 99%. Hence, if we need to make a (point) prediction, we predict that the first image is a \"7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1556371028160,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Zt2s64Sv6J8-",
    "outputId": "db36cba3-ed61-4fbe-daac-b019cc464995"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1556371039255,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "0fvXLL2t6Jzd",
    "outputId": "10f210bc-7eaf-4663-8c1f-422d57499f36"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is \"7\" and indeed the label of this image is \"7\" as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1556371056788,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "mF4obm4L8Zf8",
    "outputId": "df71cc22-2a3b-460d-d530-7e58f87b2a01"
   },
   "outputs": [],
   "source": [
    "test_labels[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first image and indeed it looks like a 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1556371087956,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Jj51sTbh8Ztc",
    "outputId": "e16c1322-1602-4435-fc59-3ac6313170b0"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_images[0],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zShpGRrSWzU"
   },
   "source": [
    "## Functions\n",
    "\n",
    "What is `relu` and `softmax`?\n",
    "\n",
    "\n",
    "The relu (rectified linear unit) function is of the form $\\max(0,x)$. Soft-max is of the form\n",
    "\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_i e^{x_i}}\n",
    "$$\n",
    "\n",
    "We can plot both functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qIrjecpSaki"
   },
   "outputs": [],
   "source": [
    "range_x = np.arange(-2,0.5,0.1)\n",
    "plt.plot(range_x,np.maximum(range_x,0),label='relu')\n",
    "plt.scatter(range_x,np.exp(range_x)/np.sum(np.exp(range_x)),label='soft-max',color='r')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will come back to neural networks in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2JkGgrO3o-i"
   },
   "source": [
    "# Overfitting and underfitting\n",
    "\n",
    "\n",
    "Adding more explanatory variables to a regression cannot make the fit worse. This is easy to see: if it would become worse, you would set the coefficient on the added variables to 0. That would give the same fit as before.\n",
    "\n",
    "A bit more formally, OLS solves the following minimization problem:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^N (y_i - \\hat y_i(\\beta))^2\n",
    "$$\n",
    "where the prediction $\\hat y_i$ is a function of the parameter values $\\beta$ that we choose. Having a larger vector $\\beta$ (more explanatory variables) cannot increase this objective function\n",
    "\n",
    "There are two problems when one does machine learning (or any other form of statistical analysis):\n",
    "* underfitting and\n",
    "* overfitting.\n",
    "\n",
    "Underfitting is the situation where a variable is important to explain $y$ but you do not include it in the regression. As most people have a tendency to include as many variables as possible, this is usually not a problem. Unless, of course, you do not have data on an important variable, but then you cannot really solve this problem.\n",
    "\n",
    "Overfitting is the situation where you include variables in your regression that are actually not relevant. The reason why this happens is that including these variables improves the fit of the model (see above and an example below). But the problem is that you are picking up correlations that are particular to the sample; not a (general) feature of the data generating process.\n",
    "\n",
    "There are a number of ways to deal with this. One is the lasso and ridge regressions we discussed above. Another is the use of an information criterion like [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) and its generalizations. The idea of AIC is to include a penalty in the expression for the fit for the number of parameters.\n",
    "\n",
    "But if you have lots of data (which is the case if you work with \"big data\"), there is a simple way to deal with this: split your data into a train and test set. You estimate the model with the train set (you \"train the model\"). And then check the fit with the test set. If you pick up particular correlations in the train set, your predictions in the test set become worse.\n",
    "\n",
    "Let's first look at an example. We generate data with a quadratic relation between $x$ and $y$. Then we generate more variables: $x^3, x^4, ..., x^9$. \n",
    "\n",
    "Including all these extra variables in training the model, improves the fit. The sum of squared residuals decreases with the number of explanatory variables in the train set. However, this is not true in the test set.\n",
    "\n",
    "First, we generate the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjffTAiHWP0I"
   },
   "outputs": [],
   "source": [
    "N_observations = 30\n",
    "train_size = 15\n",
    "x = tf.random.normal([N_observations],seed=26)\n",
    "y = x+2*x**2+2*tf.random.normal([N_observations],seed=50)\n",
    "df = pd.DataFrame({'y':y,'x':x})\n",
    "df['constant'] = 1\n",
    "df['x2'] = df.x**2\n",
    "df['x3'] = df.x**3\n",
    "df['x4'] = df.x**4\n",
    "df['x5'] = df.x**5\n",
    "df['x6'] = df.x**6\n",
    "df['x7'] = df.x**7\n",
    "df['x8'] = df.x**8\n",
    "df['x9'] = df.x**9\n",
    "df_train = df[:train_size]\n",
    "df_test = df[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset (dataframe) into a train and test set (using slicing).\n",
    "\n",
    "Then we estimate a series of regressions, starting with $y$ explained by a constant and variable $x$ and adding a term $x^n$ for $n=2,...,9$. Hence, we estimate 9 regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = 'y ~ x'\n",
    "vector_results = [smf.ols(regression, data=df_train).fit()]\n",
    "for column in df.columns[3:]:\n",
    "    regression = regression+' + '+column\n",
    "    vector_results.append(smf.ols(regression, data=df_train).fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot sum of squared residuals, we see --as claimed above-- that the fit improves with each variable $x^n$ that we add to the regression. Hence, this gives the impression that adding all 9 terms is a good idea.\n",
    "\n",
    "However, we generated the data ourselves and we know that there are no terms $x^9$ in the data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDRvPbj0_kYL"
   },
   "outputs": [],
   "source": [
    "plt.plot([tf.keras.losses.mse(df_train['y'],model.predict(df_train)).numpy() for model in vector_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the 9 models that we estimated above to predict the outcomes in the test set. For each model we plot the sum of squared residuals in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "plt.plot([np.sum((df_train['y']-model.predict(df_train))**2)/len(df_train['y']) for model in vector_results])\n",
    "plt.scatter(range(9),[tf.keras.losses.mse(df_test['y'],model.predict(df_test)).numpy() for model in vector_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit of the final model is so bad, that we cannot see the pattern in the 8 other models. Hence, for the next plot we delete the 9th model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(8),[tf.keras.losses.mse(df_test['y'],model.predict(df_test)).numpy() for model in vector_results[:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the fit does not monotonically improve with the size of the model. This due to the model picking up patterns that are present in the train sample but which do not generalize to the test sample.\n",
    "\n",
    "To understand why this happens, let's plot the train and test data points together with the prediction of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_x = np.arange(min(x),max(x),0.1)\n",
    "df_plot = pd.DataFrame({'x':range_x})\n",
    "df_plot['constant'] = 1\n",
    "df_plot['x2'] = df_plot.x**2\n",
    "df_plot['x3'] = df_plot.x**3\n",
    "df_plot['x4'] = df_plot.x**4\n",
    "df_plot['x5'] = df_plot.x**5\n",
    "df_plot['x6'] = df_plot.x**6\n",
    "df_plot['x7'] = df_plot.x**7\n",
    "df_plot['x8'] = df_plot.x**8\n",
    "df_plot['x9'] = df_plot.x**9\n",
    "plt.scatter(df_train['x'],df_train['y'],label='train data')\n",
    "plt.plot(range_x,vector_results[-1].predict(df_plot),label='prediction')\n",
    "plt.scatter(df_test['x'],df_test['y'],label='test data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the prediction is \"way off\" for low values of $x$, let's zoom in on the prediction (use a more narrow `range_x`). \n",
    "\n",
    "Now it becomes clearer what the estimation in the train set is trying to achieve. But it is also clear that this does not generalize to the test data (orange points).\n",
    "\n",
    "This is exactly what overfitting does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_x = np.arange(-1.25,1.7,0.1)\n",
    "df_plot = pd.DataFrame({'x':range_x})\n",
    "df_plot['constant'] = 1\n",
    "df_plot['x2'] = df_plot.x**2\n",
    "df_plot['x3'] = df_plot.x**3\n",
    "df_plot['x4'] = df_plot.x**4\n",
    "df_plot['x5'] = df_plot.x**5\n",
    "df_plot['x6'] = df_plot.x**6\n",
    "df_plot['x7'] = df_plot.x**7\n",
    "df_plot['x8'] = df_plot.x**8\n",
    "df_plot['x9'] = df_plot.x**9\n",
    "plt.scatter(df_train['x'],df_train['y'],label='train data')\n",
    "plt.plot(range_x,vector_results[-1].predict(df_plot),label='prediction')\n",
    "plt.scatter(df_test['x'],df_test['y'],label='test data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Make the plots of data and prediction for models 5, 6 and 7. Adjust `range_x` such that you can see \"what is going on\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "81e289dae751d3638bb24e8c57680a22",
     "grade": true,
     "grade_id": "cell-e376e9b95888e399",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f88ttpvzN6zB"
   },
   "source": [
    "Using a train and test set helps to alleviate over- and underfitting problems. But the test set should only be used once. If you repeatedly use the train and test set to see which model is best, you make the same mistake as above but \"on a higher level\": you may find patterns in the train set that generalize only to the test set but not to the data generating process.\n",
    "\n",
    "So how can you select your best model if you cannot use the test set for this? The answer is: we split the dataset in three parts: a train, validate and test set. That is, we use the train set to fit the model. Then we repeatedly use the validation set to see what the best model is. Then our final model, we test only once on the test set.\n",
    "\n",
    "This splitting of the data into three different (sub)sets is the royal road to testing the predictions of your model. Below --for ease of exposition-- we will not always to this. We tend to illustrate problems using a train and test set. We are not trying to optimize a given model by finetuning each parameter. For such finetuning, three subsets of data are recommended.\n",
    "\n",
    "Suppose you have a train and test set and your trained model does really well on the test set. How do you know that this is not by chance, e.g. due to the way you have split the data into the train and test set? One way to check is to do the analyses for a number of ways in which you can split the data into train and test sets and for each these cases consider the loss on the test set. If this is rather constant across the different ways in which you split the data, you do not need to worry. If this vary wildly between data partitions, something \"odd\" may be going on. \n",
    "\n",
    "One way of generating these different data partititions is k-fold cross-validation. See the [wikipedia page on cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_wrL0zKOM9y"
   },
   "source": [
    "# Neural network\n",
    "\n",
    "To get a first sense of how a neural network works, we look at two simple python programs that implement these algorithms. For this, we use the website from the book [machine learning: an algorithmic perspective](http://homepages.ecs.vuw.ac.nz/~marslast/MLbook.html).\n",
    "\n",
    "Below you find the code for the files [pcn_logic_eg.py](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch3/pcn_logic_eg.py) and [mlp.py](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch4/mlp.py); which we adapted for python 3 by adjusting the print statements (should be `print()` in python 3).\n",
    "\n",
    "By looking at the underlying code we get an idea how neural networks work. Then we create a neural network using tensorflow 2.0.\n",
    "\n",
    "The tensorflow syntax is very similar to [Keras](https://keras.io/). There is a [datacamp course](https://www.datacamp.com/courses/deep-learning-in-python) on deep learning with Keras and [a course on tensorflow 2.0](https://www.datacamp.com/courses/introduction-to-tensorflow-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I1uBJPHV5rJ"
   },
   "source": [
    "## Perceptron\n",
    "\n",
    "Next we copy from Chapter 3 of Machine Learning: An Algorithmic Perspective, the \"perceptron\". The code is in the next cell. Try to understand the python code.\n",
    "\n",
    "The main part of the algorithm is in the function (method, actually, but do not worry about this) `pcntrain`; the line\n",
    "\n",
    "`self.weights -= eta*np.dot(np.transpose(inputs),self.activations-targets)`\n",
    "\n",
    "determines how the weights in the network are updated. This optimization step is discussed in [this chapter of the datacamp course.](https://campus.datacamp.com/courses/deep-learning-in-python/optimizing-a-neural-network-with-backward-propagation?ex=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaWTqADydTqn"
   },
   "outputs": [],
   "source": [
    "# Code from Chapter 3 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "\n",
    "\n",
    "class pcn:\n",
    "\t\"\"\" A basic Perceptron (the same pcn.py except with the weights printed\n",
    "\tand it does not reorder the inputs)\"\"\"\n",
    "\t\n",
    "\tdef __init__(self,inputs,targets):\n",
    "\t\t\"\"\" Constructor \"\"\"\n",
    "\t\t# Set up network size\n",
    "\t\tif np.ndim(inputs)>1:\n",
    "\t\t\tself.nIn = np.shape(inputs)[1]\n",
    "\t\telse: \n",
    "\t\t\tself.nIn = 1\n",
    "\t\n",
    "\t\tif np.ndim(targets)>1:\n",
    "\t\t\tself.nOut = np.shape(targets)[1]\n",
    "\t\telse:\n",
    "\t\t\tself.nOut = 1\n",
    "\n",
    "\t\tself.nData = np.shape(inputs)[0]\n",
    "\t\n",
    "\t\t# Initialise network\n",
    "\t\tself.weights = np.random.rand(self.nIn+1,self.nOut)*0.1-0.05\n",
    "\n",
    "\tdef pcntrain(self,inputs,targets,eta,nIterations):\n",
    "\t\t\"\"\" Train the thing \"\"\"\t\n",
    "\t\t# Add the inputs that match the bias node\n",
    "\t\tinputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "\t\n",
    "\t\t# Training\n",
    "\t\tchange = range(self.nData)\n",
    "\n",
    "\t\tfor n in range(nIterations):\n",
    "\t\t\t\n",
    "\t\t\tself.activations = self.pcnfwd(inputs);\n",
    "\t\t\tself.weights -= eta*np.dot(np.transpose(inputs),self.activations-targets)\n",
    "\t\t\tprint(\"Iteration: \", n)\n",
    "\t\t\tprint(self.weights)\n",
    "\t\t\t\n",
    "\t\t\tactivations = self.pcnfwd(inputs)\n",
    "\t\t\tprint(\"Final outputs are:\")\n",
    "\t\t\tprint(activations)\n",
    "\t\t#return self.weights\n",
    "\n",
    "\tdef pcnfwd(self,inputs):\n",
    "\t\t\"\"\" Run the network forward \"\"\"\n",
    "\n",
    "\t\t# Compute activations\n",
    "\t\tactivations =  np.dot(inputs,self.weights)\n",
    "\n",
    "\t\t# Threshold the activations\n",
    "\t\treturn np.where(activations>0,1,0)\n",
    "\n",
    "\tdef confmat(self,inputs,targets):\n",
    "\t\t\"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "\t\t# Add the inputs that match the bias node\n",
    "\t\tinputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "\t\toutputs = np.dot(inputs,self.weights)\n",
    "\t\n",
    "\t\tnClasses = np.shape(targets)[1]\n",
    "\n",
    "\t\tif nClasses==1:\n",
    "\t\t\tnClasses = 2\n",
    "\t\t\toutputs = np.where(outputs>0,1,0)\n",
    "\t\telse:\n",
    "\t\t\t# 1-of-N encoding\n",
    "\t\t\toutputs = np.argmax(outputs,1)\n",
    "\t\t\ttargets = np.argmax(targets,1)\n",
    "\n",
    "\t\tcm = np.zeros((nClasses,nClasses))\n",
    "\t\tfor i in range(nClasses):\n",
    "\t\t\tfor j in range(nClasses):\n",
    "\t\t\t\tcm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "\t\tprint(cm)\n",
    "\t\tprint(np.trace(cm)/np.sum(cm))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhBrxMb3fvpZ"
   },
   "source": [
    "Below you see 4 data points with a target value (0 or 1) for each data point. We need to predict the target value. We plot this with a red color for target 0 and blue for target 1.\n",
    "\n",
    "Predicting the target here means finding a straight line such that all red points are on one side of the line and the blue points on the other side of the line. Since this example is very simple, you can draw the line yourself (I hope...). This simplicity helps us to understand what the algorithm does.\n",
    "\n",
    "We define a tensor with 4 `inputs` and 4 `targets`. The first element of `inputs` (which has two coordinates) corresponds to the first element in `targets`.\n",
    "\n",
    "To refer to the points in the figure, we use \"standard\" terminology $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1559233378416,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "ZvRsrHvKfNtj",
    "outputId": "57c888ac-dc01-4bcf-d24f-fc812ecf7be4"
   },
   "outputs": [],
   "source": [
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = np.array([[0],[1],[1],[1]])\n",
    "colors = []\n",
    "for i in range(len(targets)):\n",
    "    colors.append(['red','blue'][targets[i][0]])\n",
    "plt.scatter(inputs[:,0],inputs[:,1],color=colors)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7p2jAEXcMHT"
   },
   "source": [
    "Now we use the `pcn` class and instantiate it with the `inputs` and `targets` above. We call `p` an instance of the class `pcn`. In this course we will not dwell on the object oriented aspects of python, so do not worry if you do not fully understand this. However, as you will see below, when we use tensorflow, we have a similar structure: we create an instance of a tensorflow class `model`.\n",
    "\n",
    "The function definition is `pcntrain(self,inputs,targets,eta,nIterations)`. Do not worry about the `self` part. We call the function as `pcntrain(inputs,targets,0.25,6)`. Hence, the first two arguments of the function are our `inputs` and `targets` tensors. \n",
    "\n",
    "The result is that `p` is now associated with our tensors `inputs` and `targets`. Now we can use the function (\"method\") `pcntrain` to train the network. After each iteration, the algorithm prints some information. Looking at `pcntrain` above, we can see that the information that is printed is:\n",
    "\n",
    "```\n",
    "print(\"Iteration: \", n)\n",
    "print(self.weights)\n",
    "print(\"Final outputs are:\")\n",
    "print(activations)\n",
    "```\n",
    "\n",
    "So first we see the number of the iteration (and python starts counting at 0). Then we see three numbers that correspond to the weights. The weights determine the line in the figure above. In particular, if we write the weights tensor as $w = (w_0,w_1,w_2)$, then a line in the figure is given by $w_0 x + w_1 y =w_2$. Equivalently, we can write this as:\n",
    "\n",
    "\\begin{equation}\n",
    "y = (w_2-w_0 x)/w_1\n",
    "\\end{equation}\n",
    "\n",
    "Next, we see the final outputs (targets): this is the \"prediction\" of the algorithm for the vector `targets`. Recall that `targets` is of the form [0,1,1,1]. Hence, after the first iteration, we incorrectly label the first point as 1 while it should be 0. Hence, the algorithm continues and generates different (and ultimately better) predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1559237984092,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "63uEOtIWfN6V",
    "outputId": "45a109d8-acc7-4eb6-8cae-e35c6af42ee0"
   },
   "outputs": [],
   "source": [
    "p = pcn(inputs,targets)\n",
    "p.pcntrain(inputs,targets,0.25,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKOWDBWi0-CU"
   },
   "source": [
    "The final weights $w$ are now equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1559238001077,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "fYYxwxkFfOEU",
    "outputId": "7c823942-0cdc-4008-884b-27362a258f7d"
   },
   "outputs": [],
   "source": [
    "p.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1Bwm4bMgDf0"
   },
   "source": [
    "We can now plot the line $y=(w_2-w_0 x)/w_1$. Indeed, as you can see, the line sepates the red point (below the line) from the blue points (above the line). If we give the algorithm a point above the line, it will predict \"blue\", if we give it a point below the line it will predict \"red\". \n",
    "\n",
    "Because we only have 4 points here, this is arbitrary for points close to the line: we can shift the line, still separate the points, but get different predictions for points close to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1559238186300,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "McHF9dYYgTzN",
    "outputId": "f915397f-d00c-4f6f-848e-fedc4c609e93"
   },
   "outputs": [],
   "source": [
    "x0 = 0\n",
    "x1 = 1\n",
    "y0 = (p.weights[2]-p.weights[0]*x0)/p.weights[1]\n",
    "y1 = (p.weights[2]-p.weights[0]*x1)/p.weights[1]\n",
    "plt.scatter(inputs[:,0],inputs[:,1],color=colors)\n",
    "plt.plot([x0,x1],[y0,y1],'k') #we only need to give two points to plot a line\n",
    "plt.xlabel('$x$')\n",
    "plt.xlabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PhQ1VJw5OyE"
   },
   "source": [
    "We can see from the graph and check with the function `pcnfwd` below what the predicted labels will be for the points (0.5,0.5) and (0.5,-0.5). The `-1` is needed for the \"constant\" $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1559239211184,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "-ce5DGnTQOuN",
    "outputId": "79942d7f-3bb5-4df4-ac55-900a18473ac5"
   },
   "outputs": [],
   "source": [
    "p.pcnfwd([[0.5,0.5,-1],[0.5,-0.5,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a \"graphical\" intuition what a neural network does, see this [website](https://playground.tensorflow.org) and play around with the dataset that you want to use, the number of hidden layers, number of neurons per hidden layer and the features that you want to use.\n",
    "\n",
    "E.g. choose the dataset where the orange points \"circle\" the blue points. Use only 1 hidden layer (linear model) and 2 neurons in this layer. Then you cannot perfectly seperate the points. Play around with the options to see how you can separate the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ud5MlICH5yoE"
   },
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "With more than one layer, we get into deep learning. This is what \"deep\" in deep learning refers to: more than one layer.\n",
    "\n",
    "We use [this mlp code](http://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch4/mlp.py) from Chapter 4 of Machine Learning: An Algorithmic Perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUgtItNk5Tx9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code from Chapter 4 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class mlp:\n",
    "    \"\"\" A Multi-Layer Perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self,inputs,targets,nhidden,beta=1,momentum=0.9,outtype='logistic'):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        # Set up network size\n",
    "        self.nin = np.shape(inputs)[1]\n",
    "        self.nout = np.shape(targets)[1]\n",
    "        self.ndata = np.shape(inputs)[0]\n",
    "        self.nhidden = nhidden\n",
    "\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.outtype = outtype\n",
    "    \n",
    "        # Initialise network\n",
    "        self.weights1 = (np.random.rand(self.nin+1,self.nhidden)-0.5)*2/np.sqrt(self.nin)\n",
    "        self.weights2 = (np.random.rand(self.nhidden+1,self.nout)-0.5)*2/np.sqrt(self.nhidden)\n",
    "\n",
    "    def earlystopping(self,inputs,targets,valid,validtargets,eta,niterations=100):\n",
    "    \n",
    "        valid = np.concatenate((valid,-np.ones((np.shape(valid)[0],1))),axis=1)\n",
    "        \n",
    "        old_val_error1 = 100002\n",
    "        old_val_error2 = 100001\n",
    "        new_val_error = 100000\n",
    "        \n",
    "        count = 0\n",
    "        while (((old_val_error1 - new_val_error) > 0.001) or ((old_val_error2 - old_val_error1)>0.001)):\n",
    "            count+=1\n",
    "            print(count)\n",
    "            self.mlptrain(inputs,targets,eta,niterations)\n",
    "            old_val_error2 = old_val_error1\n",
    "            old_val_error1 = new_val_error\n",
    "            validout = self.mlpfwd(valid)\n",
    "            new_val_error = 0.5*np.sum((validtargets-validout)**2)\n",
    "            \n",
    "        print(\"Stopped\", new_val_error,old_val_error1, old_val_error2)\n",
    "        return new_val_error\n",
    "    \t\n",
    "    def mlptrain(self,inputs,targets,eta,niterations):\n",
    "        \"\"\" Train the thing \"\"\"    \n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.ndata,1))),axis=1)\n",
    "        change = range(self.ndata)\n",
    "    \n",
    "        updatew1 = np.zeros((np.shape(self.weights1)))\n",
    "        updatew2 = np.zeros((np.shape(self.weights2)))\n",
    "            \n",
    "        for n in range(niterations):\n",
    "    \n",
    "            self.outputs = self.mlpfwd(inputs)\n",
    "\n",
    "            error = 0.5*np.sum((self.outputs-targets)**2)\n",
    "            if (np.mod(n,100)==0):\n",
    "                print(\"Iteration: \",n, \" Error: \",error)\n",
    "\n",
    "            # Different types of output neurons\n",
    "            if self.outtype == 'linear':\n",
    "            \tdeltao = (self.outputs-targets)/self.ndata\n",
    "            elif self.outtype == 'logistic':\n",
    "            \tdeltao = self.beta*(self.outputs-targets)*self.outputs*(1.0-self.outputs)\n",
    "            elif self.outtype == 'softmax':\n",
    "                deltao = (self.outputs-targets)*(self.outputs*(-self.outputs)+self.outputs)/self.ndata \n",
    "            else:\n",
    "            \tprint(\"error\")\n",
    "            \n",
    "            deltah = self.hidden*self.beta*(1.0-self.hidden)*(np.dot(deltao,np.transpose(self.weights2)))\n",
    "                      \n",
    "            updatew1 = eta*(np.dot(np.transpose(inputs),deltah[:,:-1])) + self.momentum*updatew1\n",
    "            updatew2 = eta*(np.dot(np.transpose(self.hidden),deltao)) + self.momentum*updatew2\n",
    "            self.weights1 -= updatew1\n",
    "            self.weights2 -= updatew2\n",
    "                \n",
    "            # Randomise order of inputs (not necessary for matrix-based calculation)\n",
    "            #np.random.shuffle(change)\n",
    "            #inputs = inputs[change,:]\n",
    "            #targets = targets[change,:]\n",
    "            \n",
    "    def mlpfwd(self,inputs):\n",
    "        \"\"\" Run the network forward \"\"\"\n",
    "\n",
    "        self.hidden = np.dot(inputs,self.weights1);\n",
    "        self.hidden = 1.0/(1.0+np.exp(-self.beta*self.hidden))\n",
    "        self.hidden = np.concatenate((self.hidden,-np.ones((np.shape(inputs)[0],1))),axis=1)\n",
    "\n",
    "        outputs = np.dot(self.hidden,self.weights2);\n",
    "\n",
    "        # Different types of output neurons\n",
    "        if self.outtype == 'linear':\n",
    "        \treturn outputs\n",
    "        elif self.outtype == 'logistic':\n",
    "            return 1.0/(1.0+np.exp(-self.beta*outputs))\n",
    "        elif self.outtype == 'softmax':\n",
    "            normalisers = np.sum(np.exp(outputs),axis=1)*np.ones((1,np.shape(outputs)[0]))\n",
    "            return np.transpose(np.transpose(np.exp(outputs))/normalisers)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "    def confmat(self,inputs,targets):\n",
    "        \"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((np.shape(inputs)[0],1))),axis=1)\n",
    "        outputs = self.mlpfwd(inputs)\n",
    "        \n",
    "        nclasses = np.shape(targets)[1]\n",
    "\n",
    "        if nclasses==1:\n",
    "            nclasses = 2\n",
    "            outputs = np.where(outputs>0.5,1,0)\n",
    "        else:\n",
    "            # 1-of-N encoding\n",
    "            outputs = np.argmax(outputs,1)\n",
    "            targets = np.argmax(targets,1)\n",
    "\n",
    "        cm = np.zeros((nclasses,nclasses))\n",
    "        for i in range(nclasses):\n",
    "            for j in range(nclasses):\n",
    "                cm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "        print(\"Confusion matrix is:\")\n",
    "        print(cm)\n",
    "        print(\"Percentage Correct: \",np.trace(cm)/np.sum(cm)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-R0tbZB6nPk"
   },
   "source": [
    "This code is more elaborate than the perceptron above. It also introduces new concepts like the confusion matrix. We will go over these below. \n",
    "\n",
    "Below we use the famous [iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris). Admittedly, flowers are not directly linked to economics, but this is one of the classic datasets in classification. You cannot claim to have done \"datascience\" without having looked at the iris dataset.\n",
    "\n",
    "We will first download it from the website, just to learn the relevant steps here. We will work with it in numpy. In fact, the data is so famous that it is included in the tensorflow library, but later you may want to analyze data that is not included in any library. Hence, we need to learn the download steps as well.\n",
    "\n",
    "When you look at the downloaded data (in an editor), you will notice that it actually includes the names as strings. For us it is easier to work with numerical labels 0,1,2. Hence, we use the function `preprocessIris`. This function takes two arguments: the input file with the data that we downloaded and the output file where the target-labels are replaced by 0,1,2.\n",
    "\n",
    "If you run this notebook for the first time, un-comment the line `preprocessIris('iris.data','iris_proc.data')` (that is, remove the hashtag \"#\" at the beginning of the line), where `iris.data` is the name of the file that you downloaded and `iris_proc.data` is the name of the file with labels 0,1,2. Once the file `iris_proc.data` is created, there is no need to run this python command again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1558971173290,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1hiC6IOj5TqT",
    "outputId": "f00b572d-391f-420f-e0f7-ebcb57dbcd64"
   },
   "outputs": [],
   "source": [
    "# Code from Chapter 4 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2008, 2014\n",
    "\n",
    "# The iris classification example\n",
    "\n",
    "def preprocessIris(infile,outfile):\n",
    "\n",
    "    stext1 = 'Iris-setosa'\n",
    "    stext2 = 'Iris-versicolor'\n",
    "    stext3 = 'Iris-virginica'\n",
    "    rtext1 = '0'\n",
    "    rtext2 = '1'\n",
    "    rtext3 = '2'\n",
    "\n",
    "    fid = open(infile,\"r\")\n",
    "    oid = open(outfile,\"w\")\n",
    "\n",
    "    for s in fid:\n",
    "        if s.find(stext1)>-1:\n",
    "            oid.write(s.replace(stext1, rtext1))\n",
    "        elif s.find(stext2)>-1:\n",
    "            oid.write(s.replace(stext2, rtext2))\n",
    "        elif s.find(stext3)>-1:\n",
    "            oid.write(s.replace(stext3, rtext3))\n",
    "    fid.close()\n",
    "    oid.close()\n",
    "\n",
    "## Preprocessor to remove the test (only needed once)\n",
    "# preprocessIris('iris.data','iris_proc.data')\n",
    "\n",
    "iris = np.loadtxt('./data/iris_proc.data',delimiter=',')\n",
    "\n",
    "# we normalize the data (the features, not the labels):\n",
    "iris[:,:4] = iris[:,:4]-iris[:,:4].mean(axis=0)\n",
    "imax = np.concatenate((iris.max(axis=0)*np.ones((1,5)),np.abs(iris.min(axis=0)*np.ones((1,5)))),axis=0).max(axis=0)\n",
    "iris[:,:4] = iris[:,:4]/imax[:4]\n",
    "print(iris[0:5,:])\n",
    "\n",
    "\n",
    "\n",
    "# instead of using labels 0,1,2; we use labels like [1,0,0], [0,1,0], [0,0,1]; this is called one-hot encoding\n",
    "# algorithm that you use determines what the labels should look like\n",
    "# one-hot encoding is often used when analyzing text-data\n",
    "target = np.zeros((np.shape(iris)[0],3));\n",
    "indices = np.where(iris[:,4]==0) \n",
    "target[indices,0] = 1\n",
    "indices = np.where(iris[:,4]==1)\n",
    "target[indices,1] = 1\n",
    "indices = np.where(iris[:,4]==2)\n",
    "target[indices,2] = 1\n",
    "\n",
    "# Randomly order the data\n",
    "order = np.arange(np.shape(iris)[0])\n",
    "np.random.shuffle(order)\n",
    "iris = iris[order,:]\n",
    "target = target[order,:]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train = iris[::2,0:4]\n",
    "traint = target[::2]\n",
    "valid = iris[1::4,0:4]\n",
    "validt = target[1::4]\n",
    "test = iris[3::4,0:4]\n",
    "testt = target[3::4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and evaluate the following cell if you want to get an idea what the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1558971173290,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1hiC6IOj5TqT",
    "outputId": "f00b572d-391f-420f-e0f7-ebcb57dbcd64"
   },
   "outputs": [],
   "source": [
    "#print(iris)\n",
    "#print(train.max(axis=0), train.min(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1558971173290,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "1hiC6IOj5TqT",
    "outputId": "f00b572d-391f-420f-e0f7-ebcb57dbcd64"
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "#import mlp\n",
    "net = mlp(train,traint,5,outtype='logistic')\n",
    "net.earlystopping(train,traint,valid,validt,0.1)\n",
    "net.confmat(test,testt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5352
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1558977591543,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "bTRd17Ds5Teh",
    "outputId": "deb8cc99-ca09-4afb-dd19-1c8b640373d1"
   },
   "source": [
    "As the choice of train, evaluate and test data is random, the results can differ here.\n",
    "\n",
    "The idea of the confusion matrix is as follows:\n",
    "* the rows are the true labels, the columns are the predicted labels;\n",
    "* if all non-diagonal entries equal 0, the model gives a perfect prediction;\n",
    "* if the element on the 3 rd row and 1st column is positive: some observations that actually have the 3rd label are incorrectly predicted to have the 1st label.\n",
    "\n",
    "When there are a lot of labels, looking at the confusion matrix is no longer helpful. A number of summary measures have been defined for such cases, like accuracy, sensitivity, precision etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Tensorflow regression\n",
    "\n",
    "Before we do classification in tensorflow, let's quickly look at a simple regression example. In this way you can get used to the notation and syntax of tensorflow, while looking at something (regression) that you already fully understand (we hope...).\n",
    "\n",
    "This example is copied from [this SciPy 2019 Tutorial](https://www.youtube.com/watch?v=E0-mp5UlWzo) starting at 14:30.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal example of linear regression in TensorFlow 2.0, written from scratch without using any built-in layers, optimizers, or loss functions. We'll create a few points on a scatter plot, then find the best fit line using the equation `y = m * x + b`.\n",
    "\n",
    "We create our own dataset. The function `make_noisy_data` has default values for its parameters. Hence, we can call it without parameters. But we can also specify a different value, like $m = 2.0$. It is a good exercise to play around with these parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noisy_data(m=0.1, b=0.3, n=100):\n",
    "    x = tf.random.uniform(shape=(n,))\n",
    "    noise = tf.random.normal(shape=(len(x),), stddev=0.01)\n",
    "    y = m * x + b + noise\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = make_noisy_data()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8c9ca546b3bc597f8b395b64efe2c7c8",
     "grade": true,
     "grade_id": "cell-fefdf79b5453a57d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tensorflow variables `m,b` and give them a starting value. Tensorflow now knows that `m,b` are variables that will be varied (trained) later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(0.)\n",
    "b = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the variables, but they are not \"easy to look at\" as they contain more information than just the value. If we are only interested in the value, we can use the `numpy()` method on the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m)\n",
    "print(m.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that predicts `y` for given `x`. Note that tf \"knows\" that `m,b` are variables because of the way we defined these variables above. Hence, they are not explicitly mentioned in the function definition as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    y = m * x + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error measure is sum of squared differences between `y_pred` and `y_true`.\n",
    "\n",
    "To see what `reduce_mean` does, consider the following example. If the tensor `vector` has more than 1 dimension, you can specify the axis along which the reduction needs to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([[1., 2., 3., 4.]])\n",
    "print(tf.reduce_mean(vector).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Before we train the model, i.e. with our starting values for `m,b`, calculate the `squared error` that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5058b3e331c87cd970aa3e4166c9bf89",
     "grade": true,
     "grade_id": "cell-3cf0cf24392803a4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is all about reducing the loss (incorrect predictions). Most algorithms calculate the derivative of the loss function w.r.t. the parameters of the model. Then change the parameters in such a way that the loss is indeed reduced. We program a very simple gradient descent algorithm.\n",
    "\n",
    "We use gradient descent to gradually improve our guess for `m` and `b`. At each step, we'll nudge them a little bit in the right direction to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "steps = 200\n",
    "\n",
    "for i in range(steps):\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = predict(x_train)\n",
    "        loss = squared_error(predictions, y_train)\n",
    "    \n",
    "    gradients = tape.gradient(loss, [m, b])\n",
    "  \n",
    "    m.assign_sub(gradients[0] * learning_rate)\n",
    "    b.assign_sub(gradients[1] * learning_rate)\n",
    "  \n",
    "    if i % 20 == 0:    \n",
    "        print(\"Step %d, Loss %f\" % (i, loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things in the code above are new: `GradientTape` and `assign_sub`. We will discuss these briefly.\n",
    "\n",
    "More information on GradientTape can be found [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape) where you select API: r2.0 (beta), to get the right version.\n",
    "\n",
    "As a simple example to get an idea of what it does, we work with the function $z^2$. `gradient` calculates the derivative (it does so analytically) and fills in the value, in this case 3.0. Hence, the derivative equals $2*z = 6.0$ at $z=3.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.Variable(3.0)\n",
    "def my_function():\n",
    "    return z**2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    value = my_function()\n",
    "tape.gradient(value, [z])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Define the function $z_1^2+z_2^2-4z_1 z_2$ and calculate its derivatives w.r.t. $z_1,z_2$ in the point $z_1 = 1.0, z_2 = -2.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de2b471606951a176195556feb4748e0",
     "grade": true,
     "grade_id": "cell-0b4e025e6da6dab5",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign_sub` subtracts the value from the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.Variable(3.0)\n",
    "z.assign_sub(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know from the Taylor expansion of $f(x)$ around $x=a$, we can write:\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + f'(a)(x-a)\n",
    "$$\n",
    "\n",
    "Hence, if you try to minimize the function $f$, you should \"walk\" in the direction where $f$ is falling in $x$. The size of your step is determined by `learning_rate`. Big steps imply that you walk fast in the right direction, but you may jump over the minimum. With a small step size you are less likely to jump over the minimum, but it may take longer to get there.\n",
    "\n",
    "If $f'(x) >0$ and we want to minimize $f$, we should reduce $x$ to get closer to the minimum. Hence, our algorithm would do: $x_1 = x_0 - f'(a)*\\varepsilon$ where $\\varepsilon > 0$ is the learning rate.\n",
    "\n",
    "This is exactly what the code `m.assign_sub(gradients[0] * learning_rate)` does.\n",
    "\n",
    "Note that there are some issues here with local and global minima that we will not worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our regression problem, the values for the parameters that we have found are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"m: %f, b: %f\" % (m.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the regression line together with the data in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0af559f8d62464759ebdce567e7d30cf",
     "grade": true,
     "grade_id": "cell-831ce417659c9713",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor classification\n",
    "\n",
    "We first use the iris dataset to see how it works. However, this dataset is rather small, hence we move on to a bigger dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris[:,0:4]\n",
    "iris_target = iris[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(iris_features, iris_target, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our first neural network\n",
    "\n",
    "After having seen some tensorflow syntax in a simple regression example, let's look at a classification problem again.\n",
    "\n",
    "As the iris dataset is rather small, we cannot meaningfully split it up further in a train and test set. To do a proper analysis, we go back to our first neutral network that we specified above using the mnist data set on classifying handwritten numbers.\n",
    "\n",
    "We borrow parts of the analysis from [here.](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/basic_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Determine the shape of `train_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c9042f9fdf57a5602ed4fd69a4a9af25",
     "grade": true,
     "grade_id": "cell-e802e864418eafca",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how these numbers are actually coded, we plot them in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good idea to standardize your features such that they are between 0 and 1. Hence we divide by 255, because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images[0].min())\n",
    "print(train_images[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255.0\n",
    "\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some of our data and show the label for each figure below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the same model as we did before, but give some more explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hZyoNwm6Je2"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `keras` to define our deep neural network. The first line of our model, `keras.layers.Flatten`, is not a layer but specifies that we flatten our two dimensional figures into one dimensional vectors. Depending on the algorithm that you use, this step may not be necessary.\n",
    "\n",
    "After \"flattening\" our input vector is $28*28 = 784$ entries long. Our first layer has 128 nodes. The layer is \"dense\" in the sense that each of the 784 input nodes is connected to all the 128 nodes in the first layer.\n",
    "\n",
    "The activation function of this first layer is `relu`, meaning $\\max(0,x)$. So to calculate the output of this layer, we do the following: $output = W \\times input + b$ where the matrix $W$ and vector $b$ are the parameter values that we want to determine. Then the output of the first layer is $\\max(0,ouput)$ for each node. If $output \\leq 0$, the node is not activated.\n",
    "\n",
    "Then we map these 128 nodes into out final layer which is a probability distribution (`softmax`) over the 10 potential labels. The label with the highest probability is our predicted label.\n",
    "\n",
    "Then we compile our model by specifying the optimizer that we want to use; `adam` is usually a good choice. We specify the loss function that we want to minimize and we can specify other variables that we want to keep track off.\n",
    "\n",
    "To see what the options are for `model.compile`, run `model.compile?` in a cell. In the documentation, there is, for instance, a reference to `tf.keras.optimizers`. The simplest way to access this, is to use the tensorflow website (with the correct API) and look at: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate ('fit') the model on the train set. Here we specify 10 epochs.\n",
    "\n",
    "To estimate the model we usually do not want to use all the data at once as that would slow down the algorithm. Hence we take a \"minibatch\" (sample; the size of which can be set by `batch_size`) from our data and estimate the model on this sample. Then we take another sample etc.\n",
    "\n",
    "In this way, a given observation can be used in a number of samples when estimating the model. The parameter `epochs` specifies how often a given datapoint is used in a sample to estimate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23166,
     "status": "ok",
     "timestamp": 1556370965688,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "lRi2WhVB6J3i",
    "outputId": "94eeb1bc-c1ad-4a6f-d1de-29afca2d5d03"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we worry about overfitting, the fit on the training set is not very informative (unless this fit is already very bad). Hence, we use our fitted model and then evaluate it using the test data. That is, given the test_images, we predict (using the fitted model) a label; we then compare this label with the true `test_labels`.\n",
    "\n",
    "The accuracy on the test set is (as one would expect) lower than on the train set. Hence, there is some overfitting of the model on the training set. Some features of the train data are picked up by the model that do not generalize to the test data.\n",
    "\n",
    "Below we will use the prediction on the test set to determine the optimal number of epochs with an eye on overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1556370974512,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "c-0AF9lZ6J_1",
    "outputId": "6f1e7863-2b9f-4d49-8d77-246e0ae5b1d9"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for the test set and consider explicitly the prediction for the first image (index 0). This prediction gives the probability for each possible label 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1556371028160,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Zt2s64Sv6J8-",
    "outputId": "db36cba3-ed61-4fbe-daac-b019cc464995"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is the prediction for the first image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "69ae922ec9e8c91bf09455f07960bd89",
     "grade": true,
     "grade_id": "cell-6f81397facf1138e",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** If you had to make a \"point prediction\" which label (number) would you predict for the first image? Compare this to the true label of the first image in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1556371039255,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "0fvXLL2t6Jzd",
    "nbgrader": {
     "checksum": "362a2adf7acacdf44306baa8bbe0fdba",
     "grade": true,
     "grade_id": "cell-1de3db8d74c5aebc",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "10f210bc-7eaf-4663-8c1f-422d57499f36"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** As a final check, plot the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1556371087956,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "Jj51sTbh8Ztc",
    "nbgrader": {
     "checksum": "7e6287dc288cad6b97314a43150ff98d",
     "grade": true,
     "grade_id": "cell-68383cc856c456fd",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "e16c1322-1602-4435-fc59-3ac6313170b0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code \"automizes\" plotting an image together with its true and predicted label. Then we can plot a number of images, together with their predictions. If the prediction is correct, the figure is in blue, else in red.\n",
    "\n",
    "As the accuracy is quite high, you may not see any red images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n",
    "                                100*np.max(predictions_array),\n",
    "                                true_label),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, predictions[i], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we consider the problem of how to choose the number of epochs to reduce overfitting. That is, for each value of `epochs` we save the loss and accuracy in `history`. Then we plot loss and accuracy against `epochs` for the train data and the validation/test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, loss decreases (and accuracy increases) with the number of epochs on the train data. However, this is not true for the test data.\n",
    "\n",
    "For more than, say, 10 epochs, the loss on the test data hardly falls. Hence, the \"gain\" on the train data of having more than 10 epochs is due to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyDLbOdPORji"
   },
   "source": [
    "# Treatment effects\n",
    "\n",
    "Machine learning is about predicting outcomes. When the prediction is accurate (out-of-sample) we are happy, almost no matter what the model looks like. When we are trying to predict an outcome, we do not worry about the fact that one of the variables that we use, can be endogenous. As long as the variable is available when we need to predict, it is fine to use it.\n",
    "\n",
    "This is different when we are interested in policy advice, which we often are in economics. [This recent paper](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.2.87) discusses how machine learning can be used in economics. It gives examples of economic problems where prediction is, in fact, important. One such example in econometrics is instrumental variables (IV): the first stage in IV is basically generating a prediction. See [this paper](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.2.29) on how machine learning can be used to select instruments. To facilitate reading these papers, this section gives a statistical hacker's introduction to IV and estimating treatment effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsZ3XVXNOo3R"
   },
   "source": [
    "## IV\n",
    "\n",
    "To illustrate what instrumental variables are, we consider the IV example in [Richard McElreath's lecture 18\n",
    "](https://www.youtube.com/watch?v=e5cgiAGBKzI) (starting around 28:00) which is based on Angrist and Krueger (1991).\n",
    "\n",
    "You will not be surprised to hear that an individual's education (`e`) affects her/his wage (`w`). That may well be the reason that you are at university. We want to know how strong this effect is. That is, if you do a year more of education, with how much does your wage increase?\n",
    "\n",
    "A first (naive) way to analyze this is to regress years of education on wage using an individual level dataset. The problem here is a fork (see our discussion of causality above): personal motivation is likely to affect both educational achievement and wage. If someone has a high level of motivation to succeed in things, they will tend to have a high educational achievement (they are unlikely to drop out of school) and a high wage (they will be successful at their job as well). Hence, such a naive regression is likely to over-estimate the effect of education on wage.\n",
    "\n",
    "What we need is an exogenous variation in educational achievement; hence a variation that is not affected by how motivated a pupil is. In a number of countries the time you spend in school is determined by your date of birth. E.g. in the Netherlands, you go to primary school in the month where you turn 4. In other countries you can drop out of school in the month where you become 16. Hence, depending on your birth date, you spend a couple of months more (or less) in school. We assume that your month of birth is not correlated with how motivated you are when it comes to school and work.\n",
    "\n",
    "We simulate data taking this effect into account. Because we simulate the data, you can see exactly what is going on. We know the data generating process, the question is: which estimation technique is capable of uncovering the correct underlying parameters.\n",
    "\n",
    "The variable `q` captures this exogenous variation in school attendance due to birth month.\n",
    "\n",
    "We generate a dataframe `df` containing the variables `q, e, w`. \n",
    "\n",
    "**Question** Interpret the python code that generates the data. In particular, what is the role of `u` and what does `beta_ew = 0` mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcYm0YSJvF-U"
   },
   "outputs": [],
   "source": [
    "N_observations=200\n",
    "alpha_w = 1.\n",
    "beta_ew = 0.\n",
    "alpha_e = 1.\n",
    "beta_qe = 2.\n",
    "q = tf.random.uniform([N_observations])\n",
    "u = tf.random.normal([N_observations])\n",
    "e = alpha_e + beta_qe*q + u + tf.random.normal([N_observations])\n",
    "w = alpha_w + beta_ew*e + u + tf.random.normal([N_observations])\n",
    "df = pd.DataFrame({'q':q,'e':e,'w':w})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Use `smf.ols` to run the \"naive\" regression of `e` on `w`. What is the effect of education on wage? Discuss this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1556370363732,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "BQHnN4GKvr3Q",
    "nbgrader": {
     "checksum": "dcf0b6da1c3bddb575c9f24ffb9b68a4",
     "grade": true,
     "grade_id": "cell-4be9694e911ca6be",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "964e2164-d547-4e3f-bcaf-129b8b5f2ea1"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from `linearmodels.iv` we use `IV2SLS.from_formula` to run the IV, where `q` instruments for `e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1556370364979,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "PDC_1QTB2772",
    "outputId": "5e48fc3f-d239-41f4-b3e1-577a08483d58"
   },
   "outputs": [],
   "source": [
    "\n",
    "mod = IV2SLS.from_formula('w ~ 1 + [e ~ q]', df)\n",
    "\n",
    "iv_res = mod.fit()\n",
    "iv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Discuss the effect of education on wage in this regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e7710dafa95c3f7ccfdb152a263f2e3d",
     "grade": true,
     "grade_id": "cell-40e851740a0b7013",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1556370366607,
     "user": {
      "displayName": "Jan Boone",
      "photoUrl": "",
      "userId": "01680203205582182827"
     },
     "user_tz": -120
    },
    "id": "2pa4d0D25lEG",
    "outputId": "5cde68e6-7b80-4132-e1c8-103646f2974d"
   },
   "outputs": [],
   "source": [
    "iv_res.first_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Interpret the effect of `q` on `e` in the first stage regression. Is this correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "59664453d15607ab76274ad502193f37",
     "grade": true,
     "grade_id": "cell-c99715b1766b8ef1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogenous treatment effects\n",
    "\n",
    "One of the complications when using IV to find causal effects is that treatment effects can differ between subgroups and that these subgroups react differently to the treatment/instrument. Our discussion here is based on Chapter 4 of Angrist and Pischke (2009).\n",
    "\n",
    "We will consider this in the context of unemployed who are offered a training program. To understand the effects, we will again generate our own data such that we know exactly what all the relevant effects are. The question is then, how do we get these effects from the data.\n",
    "\n",
    "As before, the learning goals of this section are two fold: (i) learn how to program your own data generating process in python and (ii) understand the statistical issue under considerarion: heterogenous treatment effects.\n",
    "\n",
    "The government introduces a training program for unemployed. The treatment variable $D_i$ denotes whether (1) or not (0) individual $i$ received training. The instrument $Z_i$ that we use is whether (1) or not (0) $i$ is invited to join the training. \n",
    "\n",
    "We have three groups of unemployed:\n",
    "\n",
    "* compliers join the training if and only if they are invited to do so: $D_i = 1 (0)$ iff $Z_i = 1 (0)$;\n",
    "* always-takers join the training whether or not they are invited to do so: $D_i =1$ irrespective of $Z_i$;\n",
    "* never-takers never join the training: $D_i = 0$ irrespective of $Z_i$.\n",
    "\n",
    "Note that in this training context, the always-takers are a bit \"odd\": how can you join a training program where you are not supposed to be. The never-takers make more sense in this context: some people may not attend the training even though they were invited to do so. We will not worry about this; the point is that we can conceptually distinguish these groups.\n",
    "\n",
    "We are interested in the effect of the training program on earnings. We denote by $\\beta$ the expected earnings without training. The expected earnings with training is denoted by $\\beta+\\tau$. Hence, $\\tau$ denotes the treatment effects.\n",
    "\n",
    "Let $j \\in \\{c,a,n \\}$ denote whether an individual is a complier, always-taker or never-taker. We assume that $\\beta_j$ and $\\tau_j$ vary with $j$.\n",
    "\n",
    "In particular we assume that the earnings effect for someone in group $j$ is drawn from a normal distribution with mean $\\mu = \\beta_j$ and standard deviation $\\sigma =1$ in case of no training and the effect of training is drawn from a normal distribution with mean $\\mu = \\tau_j$ and standard deviation $\\sigma =1$.\n",
    "\n",
    "Below we will consider different configurations for $\\beta_j, \\tau_j$ and see how they affect the estimated effects.\n",
    "\n",
    "**Exercise** Read the code below and explain what it does. E.g. can you explain how earnings depend on whether someone was invited for the training program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_complier(invited):\n",
    "    return invited\n",
    "\n",
    "def accept_always(invited):\n",
    "    return np.ones_like(invited)\n",
    "\n",
    "def accept_never(invited):\n",
    "    return np.zeros_like(invited)\n",
    "\n",
    "N_simulations = 10000\n",
    "fraction_training = 0.2 # 20% of agents are invited to the training\n",
    "types = ['complier','always_taker', 'never_taker']\n",
    " = {'complier': 1, 'always_taker': 2, 'never_taker': 1}\n",
    " = {'complier': 3, 'always_taker': 4, 'never_taker': 1}\n",
    "n = {'complier': 0.5, 'always_taker': 0.25, 'never_taker': 0.25}\n",
    "accept = {'complier': accept_complier, 'always_taker': accept_always, 'never_taker': accept_never}\n",
    "earnings_without = {}\n",
    "training_effect = {}\n",
    "earnings = {}\n",
    "invited = {}\n",
    "trained = {}\n",
    "df = pd.DataFrame()\n",
    "def data_simulation( = , =, n = n):\n",
    "    for j in types:\n",
    "        earnings_without[j] = np.random.normal(loc = [j], scale = 1.0, size = int(n[j]*N_simulations))\n",
    "        training_effect[j] = np.random.normal(loc = [j], scale = 1.0, size = int(n[j]*N_simulations))\n",
    "        invited[j] = np.random.binomial(1,fraction_training,size = int(n[j]*N_simulations))\n",
    "        trained[j] = accept[j](invited[j])\n",
    "        earnings[j] = earnings_without[j]+ trained[j]*training_effect[j]\n",
    "    column_invited = np.concatenate([invited[j] for j in types],axis=0)\n",
    "    column_trained = np.concatenate([trained[j] for j in types],axis=0)\n",
    "    column_earnings = np.concatenate([earnings[j] for j in types],axis=0)\n",
    "    df = pd.DataFrame({'invited':column_invited, 'trained':column_trained,'earnings':column_earnings})\n",
    "    return df\n",
    " \n",
    "df = data_simulation()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset now with indicators for who was invited to a training and who attended a training.\n",
    "\n",
    "We would like to learn the effect of training on earnings.\n",
    "\n",
    "**Question** Which of the parameters above could we hope to recover? $\\tau_c,\\tau_a,\\tau_n$?\n",
    "\n",
    "**Question** Hence, if we do some calculations with the data, which number do we want to see?\n",
    "\n",
    "**Question** Compare the average earnings of people with training with the average earnings of people without training. Is this the number that you expected? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "972c29cbe1eb1b5f97ffd536ae75c0e6",
     "grade": true,
     "grade_id": "cell-11dfbcb44332c071",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2565d0d65888895a0eac304a316e80b0",
     "grade": true,
     "grade_id": "cell-acf4003b13766093",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Where does this number come from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3960edaad07c0d15c08fdb7f850a823a",
     "grade": true,
     "grade_id": "cell-dd52aff093968cda",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Perhaps we should use the instrument whether someone if invited or not: compare average earnings of people with and without an invitation to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "24aab6fb90e967c127e46ab591418e4b",
     "grade": true,
     "grade_id": "cell-9db7815c2398153f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Does it help if we compare:\n",
    "* people with training and an invitation to training *with*\n",
    "* people without training and no invitation to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c251646103d4a036053fb24dfb874d3e",
     "grade": true,
     "grade_id": "cell-49c080f49b9fd497",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f773bf19bba897aa231cfb8c90c5020",
     "grade": true,
     "grade_id": "cell-c821f614ca4069ce",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have seen now that we have no hope of recovering the relevant training effect in the set-up above. So the question is: when can we recover the relevant parameters? That is, what assumptions are needed?\n",
    "\n",
    "To see how you can analyze this with the set-up above, let's program two obvious cases where this works.\n",
    "\n",
    "**Question** Explain for each case *why* it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bca6bf6a7e9d35b8da14ccc7b24d61e8",
     "grade": false,
     "grade_id": "cell-3442a89a6492e55a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = {'complier': 1.0, 'always_taker': 0.0, 'never_taker': 0.0}\n",
    "df_compliers_only = data_simulation(n=n)\n",
    "np.mean(df_compliers_only[df_compliers_only.trained==1].earnings)-np.mean(df_compliers_only[df_compliers_only.trained==0].earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7f720b238236580cb600387924d5290",
     "grade": false,
     "grade_id": "cell-88d8277560e25fce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    " = {'complier': 1, 'always_taker': 1, 'never_taker': 1}\n",
    " = {'complier': 3, 'always_taker': 3, 'never_taker': 3}\n",
    "df_homog = data_simulation(=,=)\n",
    "np.mean(df_homog[df_homog.trained==1].earnings)-np.mean(df_homog[df_homog.trained==0].earnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "342655ea47f720e7780bbba1cc0b907b",
     "grade": true,
     "grade_id": "cell-8f6f6af99170937d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a less obvious case where we can retrieve relevant parameters, we will use Angrist and Pischke (2009: chapter 4). In particular, we will verify Theorem 4.4.2.\n",
    "\n",
    "We work with one sided noncompliance. That is, we allow for never-takers but not for always-takers. \n",
    "\n",
    "According to Theorem 4.4.2 the effect of treatment on the treated can be found as:\n",
    "\n",
    "$$\n",
    "\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{Prob(D_i=1|Z_i=1)}\n",
    "$$\n",
    "\n",
    "We first create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = {'complier': 0.5, 'always_taker': 0.0, 'never_taker': 0.5}\n",
    "df_one_sided = data_simulation(n=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculate $Prob(D_i = 1|Z_i =1)$ and denote this `correction_term`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "296dd73324f49824f8f5a70680587af8",
     "grade": true,
     "grade_id": "cell-e9b3b577d5be8379",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculate $\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{Prob(D_i=1|Z_i=1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4c932bc8f7ca66e806e47c25f9c7d1e",
     "grade": true,
     "grade_id": "cell-ce2c5964e6ea8f99",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which parameter have we recovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b03cf0308b269e45b0d747802e85698d",
     "grade": true,
     "grade_id": "cell-1cf09bb679b872df",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of treatment\n",
    "\n",
    "Consider a situation which is simpler than the one above in the sense that there are no always-takers, nor never-takers.\n",
    "\n",
    "However, now the instrument (\"invited\") $Z$ causes the probability of training to increase from 0.2 to 0.6. 30% of people get invited.\n",
    "\n",
    "**Exercise** Copy/paste python code from the analysis above to generate a dataframe using that $Z_i=1$ implies that the probability of treatment increases from 0.2 to 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d4bf900ee2943ac1227971df69add8a",
     "grade": true,
     "grade_id": "cell-c97483a41eb8d21f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Suppose we can observe whether someone received training or not. Use this to calculate the effect of training on earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "578369e281de4c420d4e68db2935c994",
     "grade": true,
     "grade_id": "cell-6625d613915b1528",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we cannot observe whether someone actually did the training or not (e.g. whether or not someone invested effort in the training). We only know (\"from a previous study\") that explicitly inviting someone for the training (\"nudge\") increases the probability that they invest training effort from 0.2 to 0.6.\n",
    "\n",
    "**Question** Use the dataframe above to verify what the effect of the nudge is. Call this the `correction_term`. What value should the correction term have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d08af4ee287458ca332c35f5bc18a231",
     "grade": true,
     "grade_id": "cell-556b7cd2f434cb69",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angrist and Pischke (2009: chapter 4) Theorem 4.4.1 (LATE Theorem; local average treatment effect) claims that (under some assumptions), we can calculate the effect of training on earnings as:\n",
    "\n",
    "$$\n",
    "\\frac{E(Y_i|Z_i=1)-E(Y_i|Z_i=0)}{E(D_i|Z_i=1)-E(D_i|Z_i=0)}\n",
    "$$\n",
    "\n",
    "**Question** Use this expression to calculate the effect of training on earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c94a601523d07429524894524ac0fba3",
     "grade": true,
     "grade_id": "cell-907f393d8be6a0b5",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use an IV estimation to find the same result. Run the following two regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first_stage = smf.ols('trained ~ invited', data=df_prob).fit()\n",
    "results_second_stage = smf.ols('earnings ~ invited', data=df_prob).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Use these regressions to calculate the effect of training on earning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d9e02af0f1f02c4f7386fc8f01f6bc43",
     "grade": true,
     "grade_id": "cell-85b062063df389f9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Statistical_Hacking.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
